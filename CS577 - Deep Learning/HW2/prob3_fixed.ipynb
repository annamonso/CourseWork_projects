{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "xzV1FIRrfEnV",
   "metadata": {
    "id": "xzV1FIRrfEnV"
   },
   "source": [
    "In this problem, you will implement a simplified version of the \"attention mechanism\".\n",
    "\n",
    "# A simplified version of the \"attention mechanism\"\n",
    "- The first section will describe the data.\n",
    "- The second section will describe the model.\n",
    "- The third section will be the `ag.Scalar` library\n",
    "- The fourth section will describe the problem itself.\n",
    "\n",
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rhOHPopGfJZA",
   "metadata": {
    "id": "rhOHPopGfJZA"
   },
   "source": [
    "We consider a setting where *each* sample $x^{(i)}$, where $i \\in \\{1,\\dots, n\\}$ (below, we take $n=5$, is itself a *collection* of (row) $C$ vectors (below, we will take $C = 4$), i.e.,\n",
    "$$\n",
    "x^{(i)}\n",
    "=\n",
    "\\{x^{(i,1)}, \\dots, x^{(i,C)}  \\}  \\quad \\text{where} \\quad x^{(i,c)} \\in \\mathbb{R}^d \\quad \\text{for each} \\quad c =1,\\dots, C.\n",
    "$$\n",
    "\n",
    "For a fixed $i$, we think of the $x^{(i,j)} \\in \\mathbb{R}^d$ (row) vector as a \"word embedding\". (Below, we will take $d = 3$.) So the collection $\\{x^{(i,1)}, \\dots, x^{(i,C)}  \\}$ can be viewed as a \"sentence embedding\" where each sentence has $C$ words.\n",
    "\n",
    "Alternatively, you can also view each $x^{(i)}$ as a matrix whose $c$-th row is $x^{(i,c)}$, i.e.,\n",
    "\n",
    "$$\n",
    "X^{(i)}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-& x^{(i,1)} & - \\\\ &\\vdots& \\\\\n",
    "-&x^{(i,C)} &-\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{C \\times d} \\quad \\text{ is a } C \\times d \\text{ matrix}\n",
    "$$\n",
    "\n",
    "The labels $y^{(1)},\\dots, y^{(n)} \\in \\{ \\pm 1\\}$ will indicate two classes (for example, $+1$ means \"this sentence has positive sentiment\" and $-1$ means \"this sentence has negative sentiment\").\n",
    "\n",
    "In the code, we will name\n",
    "- $C$ as `n_context`\n",
    "- $d$ as `n_features`\n",
    "- $n$ as `n_samples`\n",
    "\n",
    "The variable `X_raw` is a list (of length `n_samples`) of list (of length `n_context`) of lists (of length `n_features`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16107d04-e1d0-451a-8336-ea34525b8fe2",
   "metadata": {
    "id": "16107d04-e1d0-451a-8336-ea34525b8fe2"
   },
   "outputs": [],
   "source": [
    "n_context = 4\n",
    "n_features = 3\n",
    "n_samples = 5\n",
    "\n",
    "X_raw = [[[-0.707, -0.707, 1.0],\n",
    "          [0.963, -0.268, 1.0],\n",
    "          [0.391, 0.92, -1.0],\n",
    "          [0.899, 0.437, -1.0]],\n",
    "         [[0.327, -0.945, 1.0],\n",
    "          [0.3, -0.954, -1.0],\n",
    "          [-0.485, -0.874, -1.0],\n",
    "          [-0.694, 0.72, 1.0]],\n",
    "         [[-0.309, 0.951, -1.0],\n",
    "          [-0.951, 0.31, 1.0],\n",
    "          [-0.9, -0.437, 1.0],\n",
    "          [-0.013, -1.0, -1.0]],\n",
    "         [[0.829, -0.559, -1.0],\n",
    "          [-0.856, 0.518, 1.0],\n",
    "          [-0.2, -0.98, -1.0],\n",
    "          [-0.842, -0.539, 1.0]],\n",
    "         [[-0.938, -0.346, 1.0],\n",
    "          [-0.742, 0.67, -1.0],\n",
    "          [0.742, 0.67, -1.0],\n",
    "          [0.322, 0.947, -1.0]]]\n",
    "y_raw = [-1.0, -1.0, 1.0, 1.0, -1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537be1aa-9797-4a6c-913e-ffdec53efed5",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889fad4-7015-4ee1-ba02-4f720935c21f",
   "metadata": {},
   "source": [
    "Let $q$ be a positive integer (we will take $q=2$ in this problem) smaller than $d$.\n",
    "You will consider a model of this form:\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\theta = [ W^{(1)}, W^{(2)}, w^{(3)}]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "W^{(1)} \\text{ and }  W^{(2)} \\in \\mathbb{R}^{d \\times q}\n",
    "$$\n",
    "\n",
    "$$\n",
    " w^{(3)} \\in \\mathbb{R}^{ d} \\quad \\mbox{is a column vector}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "f(X^{(i)} ; \\theta)\n",
    "=\n",
    "\\mathrm{softmax} (x^{(i,C)} W^{(1)}  W^{(2) \\top} X^{(i) \\top} )\n",
    "X^{(i)}w^{(3)}\n",
    "$$\n",
    "\n",
    "Note:\n",
    "- $x^{(i,C)}$ is the last (bottom) row of $X^{(i)}$ and so $x^{(i,C)\\top}$ is a column vector, in particular.\n",
    "- $ X^{(i)} W^{(2)\\top } W^{(1)} x^{(i,C)^\\top}$ is a $C$-dimensional vector.\n",
    "- $f(X^{(i)} ; \\theta)$ is a scalar number.\n",
    "\n",
    "The loss function we will use is \n",
    "\n",
    "$$\n",
    "\\log( 1 + \\exp(- y^{(i)} f(X^{(i)} ; \\theta)))\n",
    "$$\n",
    "\n",
    "So the training risk is\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\log( 1 + \\exp(- y^{(i)} f(X^{(i)} ; \\theta)))\n",
    "$$\n",
    "\n",
    "This is a special form of the Cross Entropy when the number of classes is equal to $2$, and we when encode ther classes as $\\{\\pm 1\\}$ instead of $\\{1,2\\}$.\n",
    "\n",
    "In the code, we will name\n",
    "- $q$ as `n_reduced`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c40a20-b92b-41ff-9fda-4b9bc23608e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reduced = 2\n",
    "W1_raw = [[0.74, 0.529], [-0.589, 0.189], [-0.759, -0.933]] # n_features by n_reduced\n",
    "W2_raw = [[0.504, 0.651], [-0.319, -0.848], [0.606, -2.018]] # n_features by n_reduced\n",
    "w3_raw = [2.707, 0.628, 0.908] # n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16857c0-bdda-4ecd-a88a-3050116fde84",
   "metadata": {},
   "source": [
    "## The `ag.Scalar` automatic differentiation library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83469fd-7716-4562-9edc-9a71d3fa1b99",
   "metadata": {
    "id": "a83469fd-7716-4562-9edc-9a71d3fa1b99"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ag: # AutoGrad\n",
    "\n",
    "    def log(input):\n",
    "        output = ag.Scalar(math.log(input.value), inputs=[input], op=\"log\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += output.grad / input.value\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def exp(input):\n",
    "\n",
    "        output = ag.Scalar(math.exp(input.value), inputs=[input], op=\"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += output.grad * output.value\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def relu(input):\n",
    "        output = ag.Scalar(max(0, input.value), inputs=[input], op=\"relu\")\n",
    "\n",
    "        def _backward():\n",
    "            if input.value > 0:\n",
    "                input.grad += output.grad\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    class Scalar: # Scalars with grads\n",
    "        def __init__(self,\n",
    "                     value,\n",
    "                     op=\"\",\n",
    "                     _backward= lambda : None,\n",
    "                     inputs=[],\n",
    "                     label=\"\"):\n",
    "\n",
    "            self.value = float(value)\n",
    "            self.grad = 0.0\n",
    "\n",
    "            self._backward = _backward\n",
    "            self.inputs = inputs\n",
    "\n",
    "            self.op = op\n",
    "            self.label = label\n",
    "\n",
    "\n",
    "\n",
    "        def backward(self):\n",
    "            self.grad = 1.0\n",
    "\n",
    "            topo_order = self.topological_sort()\n",
    "\n",
    "            for node in reversed(topo_order):\n",
    "                node._backward()\n",
    "\n",
    "        def topological_sort(self):\n",
    "            topo_order = []\n",
    "            visited = set()\n",
    "\n",
    "            def dfs(node):\n",
    "                if node not in visited:\n",
    "                    visited.add(node)\n",
    "                    for input in node.inputs:\n",
    "                        dfs(input)\n",
    "                    topo_order.append(node)\n",
    "\n",
    "            dfs(self)\n",
    "            return topo_order\n",
    "\n",
    "\n",
    "        def __add__(self, other):\n",
    "            if not isinstance(other, ag.Scalar):\n",
    "                other = ag.Scalar(other, label=f\"{other}\\nconst\")\n",
    "\n",
    "            output = ag.Scalar(self.value + other.value,\n",
    "                               inputs=[self, other], op=\"add\")\n",
    "\n",
    "            def _backward():\n",
    "                # pass\n",
    "                self.grad += output.grad\n",
    "                other.grad += output.grad\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "\n",
    "        def __mul__(self, other):\n",
    "            if not isinstance(other, ag.Scalar):\n",
    "                other = ag.Scalar(other, label=f\"{other}\\nconst\")\n",
    "\n",
    "            output = ag.Scalar(self.value * other.value,\n",
    "                               inputs=[self, other], op=\"mul\")\n",
    "\n",
    "            def _backward():\n",
    "\n",
    "                self.grad += other.value * output.grad\n",
    "                other.grad += self.value * output.grad\n",
    "\n",
    "                return None\n",
    "\n",
    "            output._backward = _backward\n",
    "\n",
    "            return output\n",
    "        def __truediv__(self,other):\n",
    "            return self*(other**(-1))\n",
    "\n",
    "        def __neg__(self):\n",
    "            output = ag.Scalar(-self.value, inputs=[self], op=\"neg\")\n",
    "            def _backward():\n",
    "                self.grad -= output.grad\n",
    "                return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "        def __sub__(self,other):\n",
    "            return self + -other\n",
    "\n",
    "        def __pow__(self, exponent): # exponent is just a python float\n",
    "            output = ag.Scalar(self.value ** exponent, inputs=[self], op=f\"pow({exponent})\")\n",
    "\n",
    "            def _backward():\n",
    "\n",
    "                self.grad += (exponent * self.value**(exponent-1)) * output.grad\n",
    "                return None\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __repr__(self) -> str:\n",
    "            if self.op == \"\":\n",
    "                return self.label\n",
    "            else:\n",
    "                return self.label + \"\\n\" + self.op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a4eeb-975c-4dd7-bc30-91f0ffba6b85",
   "metadata": {
    "id": "7e5a4eeb-975c-4dd7-bc30-91f0ffba6b85"
   },
   "source": [
    "## Problem 3 [10 pts]\n",
    "\n",
    "Calculate using `ag.Scalar` the following: `dJdw3` `dJdW2` and `dJdW1`.\n",
    "\n",
    "Hint: \n",
    "- It will make your life easier if you build a wrapper library to support matrices and matrix multiplication, using lists of lists of `ag.Scalar`s.\n",
    "- Convert the (raw) parameter and data matrices to a matrix of `ag.Scalar`s using a function like\n",
    "  `W1 = toMat(W1_raw)`\n",
    "- Create a function like `showGrads(W1)` to help you debug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c164d",
   "metadata": {},
   "source": [
    "n_context = C = 4 (words per sentence)\n",
    "\n",
    "n_features = d = 3 (embedding size)\n",
    "\n",
    "n_reduced = q = 2 (compressed size)\n",
    "\n",
    "X(i) = (C*d) = (4x3)\n",
    "\n",
    "W(1) = (d*q) = (3*2)\n",
    "\n",
    "W(2) = (d*q) = (3*2)\n",
    "\n",
    "w(3) = (d) = (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be09a6d8-11e5-4525-a2ac-0aa09d975c11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsh9YWlC_oUt",
    "outputId": "26b9e637-2d6e-49ac-ff42-34e0709fcdb7"
   },
   "outputs": [],
   "source": [
    "# --- Helper functions for matrices of ag.Scalar ---\n",
    "\n",
    "def toMat(raw):\n",
    "    \"\"\"Convert a list of lists of floats into a list of lists of ag.Scalar.\"\"\"\n",
    "    return [[ag.Scalar(val) for val in row] for row in raw]\n",
    "\n",
    "def toVec(raw):\n",
    "    \"\"\"Convert a list of floats into a list of ag.Scalar.\"\"\"\n",
    "    return [ag.Scalar(val) for val in raw]\n",
    "\n",
    "def showGrads(mat):\n",
    "    \"\"\"Prints gradients of a matrix or vector of ag.Scalar.\"\"\"\n",
    "    if isinstance(mat[0], list):  # matrix\n",
    "        return [[x.grad for x in row] for row in mat]\n",
    "    else:  # vector\n",
    "        return [x.grad for x in mat]\n",
    "\n",
    "def scalar_sum(values):\n",
    "    total = ag.Scalar(0.0)\n",
    "    for v in values:\n",
    "        total = total + v\n",
    "    return total\n",
    "\n",
    "def dot(vec1, vec2):\n",
    "    \"\"\"Dot product of two vectors of ag.Scalar.\"\"\"\n",
    "    assert len(vec1) == len(vec2)\n",
    "    return scalar_sum(vec1[i] * vec2[i] for i in range(len(vec1)))\n",
    "\n",
    "\n",
    "def matvec_mul(mat, vec):\n",
    "    \"\"\"Matrix (m×n) times vector (n) → vector (m).\"\"\"\n",
    "    out = []\n",
    "    for i in range(len(mat)):\n",
    "        row_sum = ag.Scalar(0.0)\n",
    "        for j in range(len(vec)):\n",
    "            row_sum = row_sum + mat[i][j] * vec[j]\n",
    "        out.append(row_sum)\n",
    "    return out\n",
    "\n",
    "def matmul(A, B):\n",
    "    \"\"\"Matrix (m×n) times matrix (n×p) → matrix (m×p).\"\"\"\n",
    "    m, n, p = len(A), len(A[0]), len(B[0])\n",
    "    out = []\n",
    "    for i in range(m):\n",
    "        row = []\n",
    "        for j in range(p):\n",
    "            s = ag.Scalar(0.0)\n",
    "            for k in range(n):\n",
    "                s = s + A[i][k] * B[k][j]\n",
    "            row.append(s)\n",
    "        out.append(row)\n",
    "    return out\n",
    "\n",
    "def softmax(vec):\n",
    "    exps = [ag.exp(v) for v in vec]\n",
    "    sum_exps = scalar_sum(exps)\n",
    "    return [e / sum_exps for e in exps]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212aad4",
   "metadata": {},
   "source": [
    "### Convert data and parameters to ag.Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f430669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw data into Scalars\n",
    "X = [[toVec(word) for word in sent] for sent in X_raw]\n",
    "y = toVec(y_raw)\n",
    "\n",
    "# Convert parameters\n",
    "W1 = toMat(W1_raw)   # d × q\n",
    "W2 = toMat(W2_raw)   # d × q\n",
    "w3 = toVec(w3_raw)   # d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853f9c6",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6dbbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output forward pass 1.652897696741296\n",
      "Output forward pass -0.5030430458474209\n",
      "Output forward pass -1.5462990368860332\n",
      "Output forward pass -1.1800069124499617\n",
      "Output forward pass 0.745286662125376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# code working\n",
    "losses = []\n",
    "for i in range(n_samples):\n",
    "    Xi = X[i]             # C × d\n",
    "    xi_last = Xi[-1]      # d\n",
    "\n",
    "    # Step 1: Query = xi_last W1\n",
    "    W1_T = list(zip(*W1))  # q × d \n",
    "    XiW1 = matvec_mul(W1_T, xi_last)  # (1 x d) x (d x q) = (1 x q)\n",
    "    \n",
    "    # Step 2: Keys = Xi W2\n",
    "    XiW2 = matmul(Xi, W2)  # (C x d) * (d * q) = (C × q)\n",
    "\n",
    "    # Step 3: Scores = q1 ⋅ each row of XiW2\n",
    "    scores = [dot(XiW1, XiW2[c]) for c in range(n_context)]\n",
    "\n",
    "    # Step 4: Attention weights\n",
    "    alpha = softmax(scores) # (4,)\n",
    "\n",
    "    # Step 5: Prediction\n",
    "    Xi_w3 = [dot(Xi[c], w3) for c in range(n_context)] \n",
    "    f_i = scalar_sum(alpha[c] * Xi_w3[c] for c in range(n_context))  # scalar\n",
    "     \n",
    "    print(f\"Output forward pass {f_i.value}\")\n",
    "\n",
    "    # Step 6: Logistic loss\n",
    "    y_i = y[i]\n",
    "    loss_i = ag.log(ag.Scalar(1.0) + ag.exp(-y_i * f_i))\n",
    "    \n",
    "    losses.append(loss_i)\n",
    "\n",
    "# Average loss\n",
    "J = scalar_sum(losses) * (1.0 / n_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3df075",
   "metadata": {},
   "source": [
    "### backprop & gradient extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064319f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJdw3 = [0.2900661907172973, 0.31213455010706614, -0.22591685346105916]\n",
      "dJdW2 = [[0.05486582485602198, 0.1340124413293936], [-0.01348159334764005, -0.009055390598639454], [0.014211207749059897, 0.014788313958514373]]\n",
      "dJdW1 = [[0.02673784282047291, 0.011368294676590236], [0.04348804004907827, 0.05114747020150545], [-0.06112443715812872, -0.05294485874898042]]\n"
     ]
    }
   ],
   "source": [
    "# Backward\n",
    "J.backward()\n",
    "\n",
    "# Extract gradients\n",
    "dJdw3 = showGrads(w3)\n",
    "dJdW2 = showGrads(W2)\n",
    "dJdW1 = showGrads(W1)\n",
    "\n",
    "print(\"dJdw3 =\", dJdw3)\n",
    "print(\"dJdW2 =\", dJdW2)\n",
    "print(\"dJdW1 =\", dJdW1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1c699d-fd40-49b9-893a-6390366dd89b",
   "metadata": {
    "id": "2f1c699d-fd40-49b9-893a-6390366dd89b"
   },
   "source": [
    "## Check your answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b2edd8-d6de-45bf-aae4-aee9a9a1ac4e",
   "metadata": {
    "id": "99b2edd8-d6de-45bf-aae4-aee9a9a1ac4e"
   },
   "outputs": [],
   "source": [
    "# output of forward\n",
    "# [ 1.6528977 , -0.50304305, -1.54629904, -1.18000691,  0.74528666]\n",
    "\n",
    "# dJdw3\n",
    "# [ 0.29006619,  0.31213455, -0.22591685]\n",
    "\n",
    "# dJdW2\n",
    "# [[ 0.05486582,  0.13401244],\n",
    "#  [-0.01348159, -0.00905539],\n",
    "#  [ 0.01421121,  0.01478831]]\n",
    "\n",
    "\n",
    "# dJdW1\n",
    "# [[ 0.02673784,  0.01136829],\n",
    "#  [ 0.04348804,  0.05114747],\n",
    "#  [-0.06112444, -0.05294486]]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
