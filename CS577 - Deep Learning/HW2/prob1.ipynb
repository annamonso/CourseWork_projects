{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3fb98dd",
   "metadata": {},
   "source": [
    "# Problem 1 \n",
    "\n",
    "Draw the computational graph for the following function. Then compute wr.grad, wi.grad, and wo.grad using backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb112627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class ag: # AutoGrad\n",
    "    def log(input):\n",
    "        output = ag.Scalar(math.log(input.value), inputs=[input], op=\"log\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += output.grad / input.value\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def exp(input):\n",
    "\n",
    "        output = ag.Scalar(math.exp(input.value), inputs=[input], op=\"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += output.grad * output.value\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def relu(input):\n",
    "        output = ag.Scalar(max(0, input.value), inputs=[input], op=\"relu\")\n",
    "\n",
    "        def _backward():\n",
    "            if input.value > 0:\n",
    "                input.grad += output.grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "\n",
    "    class Scalar: # Scalars with grads\n",
    "        def __init__(self,  value, op=\"\", _backward= lambda : None, inputs=[], label=\"\"):\n",
    "\n",
    "            self.value = float(value)\n",
    "            self.grad = 0.0\n",
    "\n",
    "            self._backward = _backward\n",
    "            self.inputs = inputs\n",
    "\n",
    "            self.op = op\n",
    "            self.label = label\n",
    "\n",
    "\n",
    "        def topological_sort(self):\n",
    "            topo_order = []\n",
    "            visited = set()\n",
    "\n",
    "            def dfs(node):\n",
    "                if node not in visited:\n",
    "                    visited.add(node)\n",
    "                    for input in node.inputs:\n",
    "                        dfs(input)\n",
    "                    topo_order.append(node)\n",
    "\n",
    "            dfs(self)\n",
    "            return topo_order\n",
    "\n",
    "        def backward(self):\n",
    "            self.grad = 1.0\n",
    "\n",
    "            topo_order = self.topological_sort()\n",
    "\n",
    "            for node in reversed(topo_order):\n",
    "                node._backward()\n",
    "\n",
    "\n",
    "        def __add__(self, other):\n",
    "            if not isinstance(other, ag.Scalar):\n",
    "                other = ag.Scalar(other, label=f\"{other}\\nconst\")\n",
    "\n",
    "            output = ag.Scalar(self.value + other.value,\n",
    "                               inputs=[self, other], op=\"add\")\n",
    "\n",
    "            def _backward():\n",
    "                # pass\n",
    "                self.grad += output.grad\n",
    "                other.grad += output.grad\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "\n",
    "        def __mul__(self, other):\n",
    "            assert isinstance(other, ag.Scalar)\n",
    "            output = ag.Scalar(self.value * other.value, inputs=[self, other], op=\"mul\")\n",
    "\n",
    "            def _backward():\n",
    "                self.grad += other.value * output.grad\n",
    "                other.grad += self.value * output.grad\n",
    "\n",
    "                return None\n",
    "\n",
    "            output._backward = _backward\n",
    "\n",
    "            return output\n",
    "\n",
    "        def __pow__(self, exponent): # exponent is just a python float\n",
    "            output = ag.Scalar(self.value ** exponent, inputs=[self], op=f\"pow({exponent})\")\n",
    "\n",
    "            def _backward():\n",
    "                self.grad += (exponent * self.value**(exponent-1)) * output.grad\n",
    "                return None\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __neg__(self): # exponent is just a python float\n",
    "            output = ag.Scalar(-self.value, inputs=[self], op=f\"neg\")\n",
    "\n",
    "            def _backward():\n",
    "                self.grad += (-1) * output.grad\n",
    "                return None\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "        \n",
    "        def __repr__(self) -> str:\n",
    "            return str(\"val:\" + str(self.value) + \", grad:\" + str(self.grad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab6b593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0 12.0 22.0\n"
     ]
    }
   ],
   "source": [
    "x1 = ag.Scalar(2.0, label=\"z1\\nleaf(x1)\") \n",
    "h0 = ag.Scalar(3.0, label=\"z2\\nleaf(h0)\") \n",
    "wr = ag.Scalar(4.0, label=\"z3\\nleaf(wr)\") \n",
    "wi = ag.Scalar(5.0, label=\"z4\\nleaf(wi)\") \n",
    "wo = ag.Scalar(6.0, label=\"z5\\nleaf(wo)\")\n",
    "\n",
    "z1 = x1\n",
    "z2 = h0\n",
    "z3 = wr\n",
    "z4 = wi\n",
    "z5 = z3*z2 # wr∗h0 \n",
    "z6 = z4*z1 # wi∗x1 \n",
    "z7 = z5+z6\n",
    "z8 = ag.relu(z7) # relu(wr∗h0 + wi∗x1) \n",
    "z9 = wo\n",
    "z10 = z8*z9\n",
    "z10 . backward ()\n",
    "print(wr.grad, wi.grad, wo.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
