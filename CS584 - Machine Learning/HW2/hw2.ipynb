{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a149268",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## Anna Monso Rodriguez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f106fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "# Precision-Recall curve + AUC\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9576453b",
   "metadata": {},
   "source": [
    "### 1D 2-Class Gaussian Discriminant Analysis\n",
    "\n",
    "Dataset: Wine Quality\n",
    "\n",
    "Target : good (>=6) vs bad (<6)\n",
    "\n",
    "Feature : Alcohol\n",
    "\n",
    "**Dataset preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7eb82e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 1) (6497,)\n",
      "   alcohol  quality_bin\n",
      "0      9.4            0\n",
      "1      9.8            0\n",
      "2      9.8            0\n",
      "3      9.8            1\n",
      "4      9.4            0\n",
      "Class counts: {np.int64(0): np.int64(2384), np.int64(1): np.int64(4113)}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "data = fetch_ucirepo(id=186)  \n",
    "X_df = data.data.features\n",
    "y_df = data.data.targets\n",
    "\n",
    "y_ex1 = (y_df[\"quality\"] >= 6).astype(int).values\n",
    "\n",
    "X_ex1 = X_df[[\"alcohol\"]].values\n",
    "\n",
    "print(X_ex1.shape, y_ex1.shape)\n",
    "print(pd.DataFrame({\"alcohol\": X_ex1.flatten(), \"quality_bin\": y_ex1}).head())\n",
    "print(\"Class counts:\", dict(zip(*np.unique(y_ex1, return_counts=True))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59bcbaa",
   "metadata": {},
   "source": [
    "**Estimate parameters, compute discriminant function for each class, classify examples and measure error :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ad3a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Fold CV Results (GDA):\n",
      "Accuracy: 0.692 ± 0.018\n",
      "Precision: 0.740 ± 0.019\n",
      "Recall: 0.790 ± 0.020\n",
      "F1: 0.764 ± 0.015\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test\n",
    "X_train_ex1, X_test_ex1, y_train_ex1, y_test_ex1 = train_test_split(\n",
    "    X_ex1, y_ex1, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- GDA 1D implementation ---\n",
    "def gda_train(X, y):\n",
    "    classes = np.unique(y)\n",
    "    priors = {}\n",
    "    means = {}\n",
    "    var = 0\n",
    "    \n",
    "    n = len(y)\n",
    "    for cl in classes:\n",
    "        Xc = X[y == cl].flatten()\n",
    "        priors[cl] = len(Xc) / n\n",
    "        means[cl] = np.mean(Xc)\n",
    "        var += np.sum((Xc - means[cl])**2)\n",
    "    var /= (n - len(classes))    \n",
    "    \n",
    "    return priors, means, var\n",
    "\n",
    "def gda_predict(X, priors, means, var):\n",
    "    preds = []\n",
    "    for x in X.flatten():\n",
    "        scores = {}\n",
    "        for cl in priors:\n",
    "            mu = means[cl]\n",
    "            scores[cl] = (x * mu / var) - (0.5 * mu**2 / var) + np.log(priors[cl])\n",
    "        preds.append(max(scores, key=scores.get))\n",
    "    return np.array(preds)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "precisions, recalls, f1s, accuracies = [], [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_ex1):\n",
    "    X_train, X_test = X_ex1[train_idx], X_ex1[test_idx]\n",
    "    y_train, y_test = y_ex1[train_idx], y_ex1[test_idx]\n",
    "\n",
    "    priors, means, var = gda_train(X_train, y_train)\n",
    "    y_pred = gda_predict(X_test, priors, means, var)\n",
    "\n",
    "    precisions.append(precision_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1s.append(f1)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "gda_cv_results = {\n",
    "    \"Model\": \"Custom GDA 1D (10-fold CV)\",\n",
    "    \"Accuracy\": f\"{np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\",\n",
    "    \"Precision\": f\"{np.mean(precisions):.3f} ± {np.std(precisions):.3f}\",\n",
    "    \"Recall\": f\"{np.mean(recalls):.3f} ± {np.std(recalls):.3f}\",\n",
    "    \"F1\": f\"{np.mean(f1s):.3f} ± {np.std(f1s):.3f}\"\n",
    "}\n",
    "\n",
    "results.append(gda_cv_results)\n",
    "\n",
    "print(\"10-Fold CV Results (GDA):\")\n",
    "for k, v in gda_cv_results.items():\n",
    "    if k != \"Model\":\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78b4be",
   "metadata": {},
   "source": [
    "### nD 2-Class Gaussian Discriminant analysis \n",
    "\n",
    "**Prepare dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d2321ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (48842, 4) (48842,)\n",
      "   age  hours-per-week  fnlwgt  education-num\n",
      "0   39              40   77516             13\n",
      "1   50              13   83311             13\n",
      "2   38              40  215646              9\n",
      "3   53              40  234721              7\n",
      "4   28              40  338409             13\n",
      "Class counts: {np.int64(0): np.int64(41001), np.int64(1): np.int64(7841)}\n"
     ]
    }
   ],
   "source": [
    "# Fetch dataset (Adult Income, UCI id=2)\n",
    "data = fetch_ucirepo(id=2)\n",
    "X_df = data.data.features\n",
    "y_df = data.data.targets\n",
    "\n",
    "# Select continuous features for nD GDA\n",
    "features = [\"age\", \"hours-per-week\", \"fnlwgt\", \"education-num\"]\n",
    "X_ex2 = X_df[features].values\n",
    "\n",
    "# Convert target to binary (0: <=50K, 1: >50K)\n",
    "y_ex2 = (y_df[\"income\"] == \">50K\").astype(int).values\n",
    "\n",
    "print(\"Shape:\", X_ex2.shape, y_ex2.shape)\n",
    "print(pd.DataFrame(X_ex2, columns=features).head())\n",
    "print(\"Class counts:\", dict(zip(*np.unique(y_ex2, return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda8fc51",
   "metadata": {},
   "source": [
    "**Estimate parameters, compute discriminant function for each class, classify examples and measure error :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12b5bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Fold CV Results (GDA nD):\n",
      "Accuracy: 0.838 ± 0.005\n",
      "Precision: 0.477 ± 0.025\n",
      "Recall: 0.085 ± 0.006\n",
      "F1: 0.144 ± 0.010\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUe5JREFUeJzt3Xd0FFX/BvBnN8lueiOkEgihSo0kEEMLJRCKKFhAQZrS4RVBRToqQgARQWnKj6avSkdRIAihSIlSg0ovgdASCJDed+/vD94sWbJZskt2N5k8n3NyTnbmzsx3BnQf7r0zIxNCCBARERFJhNzSBRARERGVJYYbIiIikhSGGyIiIpIUhhsiIiKSFIYbIiIikhSGGyIiIpIUhhsiIiKSFIYbIiIikhSGGyIiIpIUhhuiSmjQoEEICAgwaJv9+/dDJpNh//79JqmpomvXrh3atWun+Xzt2jXIZDKsWbPGYjURVVYMN0RmsGbNGshkMs2Pra0t6tatizFjxiApKcnS5ZV7hUGh8Ecul8Pd3R1du3ZFbGyspcsrE0lJSfjggw9Qv3592Nvbw8HBAcHBwfjss8+QkpJi6fKIKhRrSxdAVJl8+umnqFmzJnJycnDo0CEsW7YMO3bswL///gt7e3uz1bFixQqo1WqDtmnbti2ys7OhUChMVNXTvfnmm+jWrRtUKhUuXryIpUuXon379jh27BgaN25ssbqe1bFjx9CtWzdkZGTgrbfeQnBwMADg+PHjmDNnDv744w/8/vvvFq6SqOJguCEyo65duyIkJAQAMGTIEFSpUgULFizAL7/8gjfffFPnNpmZmXBwcCjTOmxsbAzeRi6Xw9bWtkzrMFSzZs3w1ltvaT63adMGXbt2xbJly7B06VILVma8lJQU9OrVC1ZWVjh16hTq16+vtX7WrFlYsWJFmRzLFH+XiMojDksRWVCHDh0AAPHx8QAezYVxdHTElStX0K1bNzg5OaFfv34AALVajYULF6Jhw4awtbWFl5cXhg8fjocPHxbb786dOxEeHg4nJyc4OzujefPm+PHHHzXrdc25WbduHYKDgzXbNG7cGIsWLdKsL2nOzcaNGxEcHAw7Ozt4eHjgrbfewq1bt7TaFJ7XrVu30LNnTzg6OqJq1ar44IMPoFKpjL5+bdq0AQBcuXJFa3lKSgree+89+Pv7Q6lUonbt2pg7d26x3iq1Wo1FixahcePGsLW1RdWqVdGlSxccP35c02b16tXo0KEDPD09oVQq0aBBAyxbtszomp/0zTff4NatW1iwYEGxYAMAXl5emDp1quazTCbDxx9/XKxdQEAABg0apPlcOBR64MABjBo1Cp6enqhWrRo2bdqkWa6rFplMhn///Vez7Pz583jttdfg7u4OW1tbhISEYNu2bc920kQmxp4bIgsq/FKuUqWKZllBQQEiIyPRunVrzJ8/XzNcNXz4cKxZswaDBw/Gu+++i/j4eCxevBinTp3C4cOHNb0xa9aswdtvv42GDRti0qRJcHV1xalTpxAdHY2+ffvqrGP37t1488030bFjR8ydOxcAcO7cORw+fBhjx44tsf7Cepo3b46oqCgkJSVh0aJFOHz4ME6dOgVXV1dNW5VKhcjISISGhmL+/PnYs2cPvvjiC9SqVQsjR4406vpdu3YNAODm5qZZlpWVhfDwcNy6dQvDhw9H9erVceTIEUyaNAl37tzBwoULNW3feecdrFmzBl27dsWQIUNQUFCAgwcP4s8//9T0sC1btgwNGzbESy+9BGtra/z6668YNWoU1Go1Ro8ebVTdRW3btg12dnZ47bXXnnlfuowaNQpVq1bF9OnTkZmZie7du8PR0REbNmxAeHi4Vtv169ejYcOGaNSoEQDgzJkzaNWqFfz8/DBx4kQ4ODhgw4YN6NmzJzZv3oxevXqZpGaiZyaIyORWr14tAIg9e/aIe/fuiRs3boh169aJKlWqCDs7O3Hz5k0hhBADBw4UAMTEiRO1tj948KAAIH744Qet5dHR0VrLU1JShJOTkwgNDRXZ2dlabdVqteb3gQMHiho1amg+jx07Vjg7O4uCgoISz2Hfvn0CgNi3b58QQoi8vDzh6ekpGjVqpHWs3377TQAQ06dP1zoeAPHpp59q7fP5558XwcHBJR6zUHx8vAAgPvnkE3Hv3j2RmJgoDh48KJo3by4AiI0bN2razpw5Uzg4OIiLFy9q7WPixInCyspKJCQkCCGE2Lt3rwAg3n333WLHK3qtsrKyiq2PjIwUgYGBWsvCw8NFeHh4sZpXr16t99zc3NxE06ZN9bYpCoCYMWNGseU1atQQAwcO1Hwu/DvXunXrYn+ub775pvD09NRafufOHSGXy7X+jDp27CgaN24scnJyNMvUarVo2bKlqFOnTqlrJjI3DksRmVFERASqVq0Kf39/vPHGG3B0dMTWrVvh5+en1e7JnoyNGzfCxcUFnTp1QnJysuYnODgYjo6O2LdvH4BHPTDp6emYOHFisfkxMpmsxLpcXV2RmZmJ3bt3l/pcjh8/jrt372LUqFFax+revTvq16+P7du3F9tmxIgRWp/btGmDq1evlvqYM2bMQNWqVeHt7Y02bdrg3Llz+OKLL7R6PTZu3Ig2bdrAzc1N61pFRERApVLhjz/+AABs3rwZMpkMM2bMKHacotfKzs5O83tqaiqSk5MRHh6Oq1evIjU1tdS1lyQtLQ1OTk7PvJ+SDB06FFZWVlrL+vTpg7t372oNMW7atAlqtRp9+vQBADx48AB79+5F7969kZ6errmO9+/fR2RkJC5dulRs+JGovOCwFJEZLVmyBHXr1oW1tTW8vLxQr149yOXa/8awtrZGtWrVtJZdunQJqamp8PT01Lnfu3fvAng8zFU4rFBao0aNwoYNG9C1a1f4+fmhc+fO6N27N7p06VLiNtevXwcA1KtXr9i6+vXr49ChQ1rLCue0FOXm5qY1Z+jevXtac3AcHR3h6Oio+Txs2DC8/vrryMnJwd69e/HVV18Vm7Nz6dIl/P3338WOVajotfL19YW7u3uJ5wgAhw8fxowZMxAbG4usrCytdampqXBxcdG7/dM4OzsjPT39mfahT82aNYst69KlC1xcXLB+/Xp07NgRwKMhqaCgINStWxcAcPnyZQghMG3aNEybNk3nvu/evVssmBOVBww3RGbUokULzVyOkiiVymKBR61Ww9PTEz/88IPObUr6Ii8tT09PxMXFYdeuXdi5cyd27tyJ1atXY8CAAVi7du0z7bvQk70HujRv3lwTmoBHPTVFJ8/WqVMHERERAIAXX3wRVlZWmDhxItq3b6+5rmq1Gp06dcKECRN0HqPwy7s0rly5go4dO6J+/fpYsGAB/P39oVAosGPHDnz55ZcG306vS/369REXF4e8vLxnus2+pInZRXueCimVSvTs2RNbt27F0qVLkZSUhMOHD2P27NmaNoXn9sEHHyAyMlLnvmvXrm10vUSmxHBDVAHUqlULe/bsQatWrXR+WRVtBwD//vuvwV88CoUCPXr0QI8ePaBWqzFq1Ch88803mDZtms591ahRAwBw4cIFzV1fhS5cuKBZb4gffvgB2dnZms+BgYF620+ZMgUrVqzA1KlTER0dDeDRNcjIyNCEoJLUqlULu3btwoMHD0rsvfn111+Rm5uLbdu2oXr16prlhcOAZaFHjx6IjY3F5s2bS3wcQFFubm7FHuqXl5eHO3fuGHTcPn36YO3atYiJicG5c+cghNAMSQGPr72Njc1TryVRecM5N0QVQO/evaFSqTBz5sxi6woKCjRfdp07d4aTkxOioqKQk5Oj1U4IUeL+79+/r/VZLpejSZMmAIDc3Fyd24SEhMDT0xPLly/XarNz506cO3cO3bt3L9W5FdWqVStERERofp4WblxdXTF8+HDs2rULcXFxAB5dq9jYWOzatatY+5SUFBQUFAAAXn31VQgh8MknnxRrV3itCnubil671NRUrF692uBzK8mIESPg4+OD999/HxcvXiy2/u7du/jss880n2vVqqWZN1To22+/NfiW+oiICLi7u2P9+vVYv349WrRooTWE5enpiXbt2uGbb77RGZzu3btn0PGIzIk9N0QVQHh4OIYPH46oqCjExcWhc+fOsLGxwaVLl7Bx40YsWrQIr732GpydnfHll19iyJAhaN68Ofr27Qs3NzecPn0aWVlZJQ4xDRkyBA8ePECHDh1QrVo1XL9+HV9//TWCgoLw3HPP6dzGxsYGc+fOxeDBgxEeHo4333xTcyt4QEAAxo0bZ8pLojF27FgsXLgQc+bMwbp16/Dhhx9i27ZtePHFFzFo0CAEBwcjMzMT//zzDzZt2oRr167Bw8MD7du3R//+/fHVV1/h0qVL6NKlC9RqNQ4ePIj27dtjzJgx6Ny5s6ZHa/jw4cjIyMCKFSvg6elpcE9JSdzc3LB161Z069YNQUFBWk8oPnnyJH766SeEhYVp2g8ZMgQjRozAq6++ik6dOuH06dPYtWsXPDw8DDqujY0NXnnlFaxbtw6ZmZmYP39+sTZLlixB69at0bhxYwwdOhSBgYFISkpCbGwsbt68idOnTz/byROZiiVv1SKqLApvyz127JjedgMHDhQODg4lrv/2229FcHCwsLOzE05OTqJx48ZiwoQJ4vbt21rttm3bJlq2bCns7OyEs7OzaNGihfjpp5+0jlP0VvBNmzaJzp07C09PT6FQKET16tXF8OHDxZ07dzRtnrwVvND69evF888/L5RKpXB3dxf9+vXT3Nr+tPOaMWOGKM3/hgpvq/788891rh80aJCwsrISly9fFkIIkZ6eLiZNmiRq164tFAqF8PDwEC1bthTz588XeXl5mu0KCgrE559/LurXry8UCoWoWrWq6Nq1qzhx4oTWtWzSpImwtbUVAQEBYu7cuWLVqlUCgIiPj9e0M/ZW8EK3b98W48aNE3Xr1hW2trbC3t5eBAcHi1mzZonU1FRNO5VKJT766CPh4eEh7O3tRWRkpLh8+XKJt4Lr+zu3e/duAUDIZDJx48YNnW2uXLkiBgwYILy9vYWNjY3w8/MTL774oti0aVOpzovIEmRC6OmrJiIiIqpgOOeGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkpdI9xE+tVuP27dtwcnLS+5ZkIiIiKj+EEEhPT4evr2+x9+89qdKFm9u3b8Pf39/SZRAREZERbty4gWrVqultU+nCjZOTE4BHF8fZ2dnC1RAREVFppKWlwd/fX/M9rk+lCzeFQ1HOzs4MN0RERBVMaaaUcEIxERERSQrDDREREUkKww0RERFJSqWbc0NEROWbSqVCfn6+pcsgC1AoFE+9zbs0GG6IiKhcEEIgMTERKSkpli6FLEQul6NmzZpQKBTPtB+GGyIiKhcKg42npyfs7e35oNVKpvAhu3fu3EH16tWf6c+f4YaIiCxOpVJpgk2VKlUsXQ5ZSNWqVXH79m0UFBTAxsbG6P1wQjEREVlc4Rwbe3t7C1dCllQ4HKVSqZ5pPww3RERUbnAoqnIrqz9/hhsiIiKSFIuGmz/++AM9evSAr68vZDIZfv7556dus3//fjRr1gxKpRK1a9fGmjVrTF4nERERVRwWDTeZmZlo2rQplixZUqr28fHx6N69O9q3b4+4uDi89957GDJkCHbt2mXiSomIiPSLjY2FlZUVunfvrrV8//79kMlkOm9xDwgIwMKFC7WW7du3D926dUOVKlVgb2+PBg0a4P3338etW7eMrm3JkiUICAiAra0tQkNDcfToUb3tt2zZgpCQELi6usLBwQFBQUH4/vvvtdoMGjQIMplM66dLly4695ebm4ugoCDIZDLExcUZfR6lZdFw07VrV3z22Wfo1atXqdovX74cNWvWxBdffIHnnnsOY8aMwWuvvYYvv/zSxJU+XW6BCjcfZiE1iw+eIiKqjFauXIn//Oc/+OOPP3D79m2j9vHNN98gIiIC3t7e2Lx5M86ePYvly5cjNTUVX3zxhVH7XL9+PcaPH48ZM2bg5MmTaNq0KSIjI3H37t0St3F3d8eUKVMQGxuLv//+G4MHD8bgwYOLdSZ06dIFd+7c0fz89NNPOvc3YcIE+Pr6GlW/MSrUreCxsbGIiIjQWhYZGYn33nuvxG1yc3ORm5ur+ZyWlmaS2s7cTsMrS4/AWi7Dd++0QMtaHiY5DhERlT8ZGRlYv349jh8/jsTERKxZswaTJ082aB83b97Eu+++i3fffVfrH+0BAQFo27at0Q83XLBgAYYOHYrBgwcDeNRRsH37dqxatQoTJ07UuU27du20Po8dOxZr167FoUOHEBkZqVmuVCrh7e2t9/g7d+7E77//js2bN2Pnzp1GnYOhKtSE4sTERHh5eWkt8/LyQlpaGrKzs3VuExUVBRcXF82Pv7+/SWornN9doBY4e9s0AYqIqDIRQiArr8DsP0IIg2vdsGED6tevj3r16uGtt97CqlWrDN7Pxo0bkZeXhwkTJuhc7+rqCgBISEiAo6Oj3p/Zs2cDAPLy8nDixAmtjgG5XI6IiAjExsaWqi4hBGJiYnDhwgW0bdtWa93+/fvh6emJevXqYeTIkbh//77W+qSkJAwdOhTff/+9WW/zr1A9N8aYNGkSxo8fr/mclpZmkoDzfHU39Azyxc9xxnVFEhGRtux8FRpMN/+cyrOfRsJeYdjX48qVK/HWW28BeDRUk5qaigMHDhTrAdHn0qVLcHZ2ho+Pj952vr6+T5234u7uDgBITk6GSqXS2TFw/vx5vftITU2Fn58fcnNzYWVlhaVLl6JTp06a9V26dMErr7yCmjVr4sqVK5g8eTK6du2qmXskhMCgQYMwYsQIhISE4Nq1a3qPV5YqVLjx9vZGUlKS1rKkpCQ4OzvDzs5O5zZKpRJKpdIc5RERUSV04cIFHD16FFu3bgUAWFtbo0+fPli5cqVB4UYIUarnvFhbW6N27drGlltqTk5OiIuLQ0ZGBmJiYjB+/HgEBgZqzumNN97QtG3cuDGaNGmCWrVqYf/+/ejYsSO+/vprpKenY9KkSSav9UkVKtyEhYVhx44dWst2796NsLAwC1VERESmYmdjhbOfRj69oQmOa4iVK1eioKBAa8KsEAJKpRKLFy+Gs7MzgEc9IYVDS4VSUlLg4uICAKhbty5SU1Nx584dvb03CQkJaNCggd6aJk+ejMmTJ8PDwwNWVlY6OwaeNldGLpdrQlRQUBDOnTuHqKioEgNbYGAgPDw8cPnyZXTs2BF79+5FbGxssQ6GkJAQ9OvXD2vXrtV7/Gdh0XCTkZGBy5cvaz7Hx8cjLi4O7u7uqF69OiZNmoRbt27hu+++AwCMGDECixcvxoQJE/D2229j79692LBhA7Zv326pUyAiIhORyWQGDw+ZW0FBAb777jt88cUX6Ny5s9a6nj174qeffkK/fv0gl8tx4sQJ1KhRQ7P+6tWrSE1NRd26dQEAr732GiZOnIh58+bpvAs4JSUFrq6uBg1LKRQKBAcHIyYmBj179gTw6AWVMTExGDNmjEHnqlartW7QedLNmzdx//59TTD76quv8Nlnn2nW3759G5GRkVi/fj1CQ0MNOrbBhAXt27dPACj2M3DgQCGEEAMHDhTh4eHFtgkKChIKhUIEBgaK1atXG3TM1NRUAUCkpqaWzUkUMfank6LGR7+JFX9cKfN9ExFJWXZ2tjh79qzIzs62dCkG2bp1q1AoFCIlJaXYugkTJoiQkBAhhBDDhg0TAQEB4pdffhFXr14VBw4cEC+88IJ44YUXhFqt1myzZMkSIZPJxNtvvy32798vrl27Jg4dOiSGDRsmxo8fb1SN69atE0qlUqxZs0acPXtWDBs2TLi6uorExERNm/79+4uJEydqPs+ePVv8/vvv4sqVK+Ls2bNi/vz5wtraWqxYsUIIIUR6err44IMPRGxsrIiPjxd79uwRzZo1E3Xq1BE5OTk664iPjxcAxKlTp0qsVd/fA0O+vy0aidu1a6d3Nrmupw+3a9cOp06dMmFVREREpbNy5UpERERohpaKevXVVzFv3jz8/fffWLRoEebMmYOPPvoI169fh7e3Nzp16oRZs2ZpzbMZNWoU6tati/nz56NXr17Izs5GQEAAXnzxRa2bYwzRp08f3Lt3D9OnT0diYiKCgoIQHR2tNck4ISEBcvnjG6gzMzMxatQo3Lx5E3Z2dqhfvz7++9//ok+fPgAAKysr/P3331i7di1SUlLg6+uLzp07Y+bMmeVinqtM6EsXEpSWlgYXFxekpqZqxkHLynvrTuHnuNuY2v05DGkTWKb7JiKSspycHMTHx6NmzZqwtbW1dDlkIfr+Hhjy/V2hnnNDRERE9DQMN0RERCQpDDdEREQkKQw3REREJCkMN0REVG5Usntc6All9efPcENERBZnY2MDAMjKyrJwJWRJeXl5AB7dav4syvejH4mIqFKwsrKCq6sr7t69CwCwt7cv1XuWSDrUajXu3bsHe3t7WFs/WzxhuCEionKh8F1HhQGHKh+5XI7q1as/c7BluCEionJBJpPBx8cHnp6eyM/Pt3Q5ZAEKhULrScnGYrghIqJyxcrK6pnnXFDlxgnFREREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkWDzdLlixBQEAAbG1tERoaiqNHj+ptv3DhQtSrVw92dnbw9/fHuHHjkJOTY6ZqiYiIqLyzaLhZv349xo8fjxkzZuDkyZNo2rQpIiMjcffuXZ3tf/zxR0ycOBEzZszAuXPnsHLlSqxfvx6TJ082c+VERERUXlk03CxYsABDhw7F4MGD0aBBAyxfvhz29vZYtWqVzvZHjhxBq1at0LdvXwQEBKBz58548803n9rbQ0RERJWHxcJNXl4eTpw4gYiIiMfFyOWIiIhAbGyszm1atmyJEydOaMLM1atXsWPHDnTr1q3E4+Tm5iItLU3rh4iIiKTL2lIHTk5OhkqlgpeXl9ZyLy8vnD9/Xuc2ffv2RXJyMlq3bg0hBAoKCjBixAi9w1JRUVH45JNPyrR2IiIiKr8sPqHYEPv378fs2bOxdOlSnDx5Elu2bMH27dsxc+bMEreZNGkSUlNTNT83btwwY8VERERkbhbrufHw8ICVlRWSkpK0liclJcHb21vnNtOmTUP//v0xZMgQAEDjxo2RmZmJYcOGYcqUKZDLi2c1pVIJpVJZ9idARERE5ZLFem4UCgWCg4MRExOjWaZWqxETE4OwsDCd22RlZRULMFZWVgAAIYTpiiUiIqIKw2I9NwAwfvx4DBw4ECEhIWjRogUWLlyIzMxMDB48GAAwYMAA+Pn5ISoqCgDQo0cPLFiwAM8//zxCQ0Nx+fJlTJs2DT169NCEHCIiIqrcLBpu+vTpg3v37mH69OlITExEUFAQoqOjNZOMExIStHpqpk6dCplMhqlTp+LWrVuoWrUqevTogVmzZlnqFIiIiKickYlKNp6TlpYGFxcXpKamwtnZuUz3/d66U/g57jamdn8OQ9oElum+iYiIKjNDvr8r1N1SRERERE/DcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREkmJtzEYqlQpr1qxBTEwM7t69C7VarbV+7969ZVIcERERkaGMCjdjx47FmjVr0L17dzRq1Agymays6yIiIiIyilHhZt26ddiwYQO6detW1vUQERERPROj5twoFArUrl27rGshIiIiemZGhZv3338fixYtghCirOshIiIieiZGDUsdOnQI+/btw86dO9GwYUPY2Nhord+yZUuZFEdERERkKKPCjaurK3r16lXWtRARERE9M6PCzerVq8u6DiIiIqIyYVS4KXTv3j1cuHABAFCvXj1UrVq1TIoiIiIiMpZRE4ozMzPx9ttvw8fHB23btkXbtm3h6+uLd955B1lZWWVdIxEREVGpGRVuxo8fjwMHDuDXX39FSkoKUlJS8Msvv+DAgQN4//33y7pGIiIiolIzalhq8+bN2LRpE9q1a6dZ1q1bN9jZ2aF3795YtmxZWdVHREREZBCjem6ysrLg5eVVbLmnpyeHpYiIiMiijAo3YWFhmDFjBnJycjTLsrOz8cknnyAsLKzMiiMiIiIylFHDUosWLUJkZCSqVauGpk2bAgBOnz4NW1tb7Nq1q0wLJCIiIjKEUT03jRo1wqVLlxAVFYWgoCAEBQVhzpw5uHTpEho2bGjQvpYsWYKAgADY2toiNDQUR48e1ds+JSUFo0ePho+PD5RKJerWrYsdO3YYcxpEREQkQUY/58be3h5Dhw59poOvX78e48ePx/LlyxEaGoqFCxciMjISFy5cgKenZ7H2eXl56NSpEzw9PbFp0yb4+fnh+vXrcHV1faY6iIiISDpKHW62bduGrl27wsbGBtu2bdPb9qWXXirVPhcsWIChQ4di8ODBAIDly5dj+/btWLVqFSZOnFis/apVq/DgwQMcOXJE8z6rgICA0p6C2WTmqixdAhERUaVV6nDTs2dPJCYmwtPTEz179iyxnUwmg0r19C/3vLw8nDhxApMmTdIsk8vliIiIQGxsrM5ttm3bhrCwMIwePRq//PILqlatir59++Kjjz6ClZWVzm1yc3ORm5ur+ZyWlvbU2p7Vl3suwtZGjuHhtUx+LCIiItJW6jk3arVaM1SkVqtL/ClNsAGA5ORkqFSqYreUe3l5ITExUec2V69exaZNm6BSqbBjxw5MmzYNX3zxBT777LMSjxMVFQUXFxfNj7+/fynP+NlE7TxvluMQERGRNqMmFOuSkpJSVrsqUWHA+vbbbxEcHIw+ffpgypQpWL58eYnbTJo0CampqZqfGzdumLxOIiIishyjws3cuXOxfv16zefXX38d7u7u8PPzw+nTp0u1Dw8PD1hZWSEpKUlreVJSEry9vXVu4+Pjg7p162oNQT333HNITExEXl6ezm2USiWcnZ21foiIiEi6jAo3y5cv1wzv7N69G3v27EF0dDS6du2KDz/8sFT7UCgUCA4ORkxMjGaZWq1GTExMiQ8CbNWqFS5fvgy1Wq1ZdvHiRfj4+EChUBhzKkRERCQxRoWbxMRETbj57bff0Lt3b3Tu3BkTJkzAsWPHSr2f8ePHY8WKFVi7di3OnTuHkSNHIjMzU3P31IABA7QmHI8cORIPHjzA2LFjcfHiRWzfvh2zZ8/G6NGjjTkNIiIikiCjnnPj5uaGGzduwN/fH9HR0ZoJvUKIUk8oBoA+ffrg3r17mD59OhITExEUFITo6GjNJOOEhATI5Y/zl7+/P3bt2oVx48ahSZMm8PPzw9ixY/HRRx8ZcxpEREQkQUaFm1deeQV9+/ZFnTp1cP/+fXTt2hUAcOrUKdSuXdugfY0ZMwZjxozRuW7//v3FloWFheHPP/80uGYiIiKqHIwKN19++SUCAgJw48YNzJs3D46OjgCAO3fuYNSoUWVaIBEREZEhjAo3NjY2+OCDD4otHzdu3DMXRERERPQsLPr6BSIiIqKyZrHXLxARERGZQqnDTdFnyxT9nR67l5H79EZERERkUmX2+gXi28CJiIjKA6PCzbvvvouvvvqq2PLFixfjvffee9aaKiwhRLFleQVq5OQz9BAREZmLUeFm8+bNaNWqVbHlLVu2xKZNm565qIpK9US4UasFQmfvwQtRMShQcSiPiIjIHIwKN/fv34eLi0ux5c7OzkhOTn7moiqqovlFLgNSsvPxMCsfKVn5uPkw23KFERERVSJGhZvatWsjOjq62PKdO3ciMDDwmYuqqIoOS1nL5cjMLdB8Xnko3hIlERERVTpGPcRv/PjxGDNmDO7du4cOHToAAGJiYvDFF19g4cKFZVlfhaJSPw43cjmQlfd4rs2tFPbcEBERmYNR4ebtt99Gbm4uZs2ahZkzZwIAAgICsGzZMgwYMKBMC6xI1E/23OQ97rlxVBp1qYmIiMhARn/jjhw5EiNHjsS9e/dgZ2eneb9UZVak4wZWcpnWsFQ275giIiIyC6Ofc1NQUIA9e/Zgy5Ytmrkmt2/fRkZGRpkVV9EUHZaylsu0nnuz+2wS1h1NsERZRERElYpRPTfXr19Hly5dkJCQgNzcXHTq1AlOTk6YO3cucnNzsXz58rKus0IoOiwll8uQVWRYCgCmbzuDN1pUN3dZRERElYpRPTdjx45FSEgIHj58CDs7O83yXr16ISYmpsyKq2jUT/Tc5D/xbBtnWxtzl0RERFTpGNVzc/DgQRw5cgQKhUJreUBAAG7dulUmhVVET865yXjidQzOdpxUTEREZGpG9dyo1Wqdb/6+efMmnJycnrmoiqroE4qt5DJk5WoPS7HnhoiIyPSMCjedO3fWep6NTCZDRkYGZsyYgW7dupVVbRVO0WGp6/ezkJmnHQAfZOaZuyQiIqJKx6hxkvnz56NLly5o0KABcnJy0LdvX1y6dAkeHh746aefyrrGCkP9xLuljl97oPU54UGWOcshIiKqlIwKN/7+/jh9+jTWr1+P06dPIyMjA++88w769eunNcG4sil6KzgAnLmdZqFKiIiIKi+Dw01+fj7q16+P3377Df369UO/fv1MUVeF9ETHDXIL9D+4LydfBblMBoW10Y8bIiIioicY/K1qY2ODnJwcU9RS4ameSDdPdORoyc5T4flPd+OlxYdMXBUREVHlYlSXwejRozF37lwUFBQ8vXEl8uScG31OXH+I7HwVziemm7AiIiKiyseoOTfHjh1DTEwMfv/9dzRu3BgODg5a67ds2VImxVU0avXT26Rk5cHVXoHbqY/fEi6EgEwmM2FlRERElYdR4cbV1RWvvvpqWddS4T05LKVLbsGjBHQn5fHQ3spD8RjSJtBkdREREVUmBoUbtVqNzz//HBcvXkReXh46dOiAjz/+uFLfIVVUaYalrOWPemhupzzuufls+zmGGyIiojJi0JybWbNmYfLkyXB0dISfnx+++uorjB492lS1VTgGTLnRGpYCHk0wJiIiomdnULj57rvvsHTpUuzatQs///wzfv31V/zwww9Ql2ayCQEALvxvAvGdVO07zpbtv2yJcoiIiCTHoHCTkJCg9XqFiIgIyGQy3L59u8wLk5KT0zppfp//+wUAwK2H2j03R594mjEREREZx6BwU1BQAFtbW61lNjY2yM/PL9OipMbF7vELM51sbSCEQHa+9jBUdXd7c5dFREQkSQZNKBZCYNCgQVAqlZplOTk5GDFihNbt4JX1VvCSWMkf3+btZm+D+zpeoOnmoDBnSURERJJlULgZOHBgsWVvvfVWmRVTGbg7KPF59IViy785cBVvNK+Omh4OOrYiIiKi0jIo3KxevdpUdVQaz/k44cNNf+tct+XkTbzfuZ6ZKyIiIpIWvrHRxF4LrgYACK9bFQCKPYnYz/XxM4L8Oe+GiIjomTHcmJitzaNLnPO/CcSrDsVrrb9V5GF+RSceExERkXEYbkzM1toKAPBX/KNbvc/eSdOsWz24uVbbuTvPm68wIiIiiWK4MTE7hVWJ65pWc0W/0Oqaz1eTM81REhERkaQx3JiYrU3J4cbN3gazejU2YzVERETSx3BjYkrrki9x4eTi53ycAQCvNPMzS01ERERSxnBjYvp6bgpFNvQCANjrGcIiIiKi0mG4MbHCcPPL6FYWroSIiKhyYLgxMbv/hZt8lfab0+e92qRY2//+mYAHmXma28aJiIjIcAw3Jlb4nJtGfi5ay2t5Omp+v3rv8V1SzWbuxgtRMUjL0X4Z6Znbqdhw7AaEECasloiIqOIz6PULZLjCYakn5954Oj1++ei207e11qVk5SMuIQVt//dU43d/OqVpU72KPV4IrGLKkomIiCo09tyYWGHPzZM8HB+HG113VFVxfPSW8Dup2VrhJz2noIwrJCIikhaGGxMr6W6pog/3e7djnWLrZXh0m/iJ6w+1ljvZsrONiIhIH4YbEyvNreCv/+/lmk/KyVdhzI+nnrmG9Jx8xF65j+m//IuM3AJk5BZgxR9XkZSWU+p93HiQhec//R1L9l1+5nqIiIhMid0AJlY03PQOqYYNx29CYaWdKT2dbVHFQYH7mXlay9cfu/HMx7+Xnovms/ZoPn8Xe13z+6wd5/BN/2BENvTWu/2I/57Q9CB9vusC3m5VU+9rJYiIiCyJPTcm5lxkGGlWr8bYPLIlznwaWazdgQntiy27k1q8Z+WNb//E5bsZOo+VV6BGQZFbzlVqoRVsdBn+/YkS1xVu/+TQWMs5MZj2878ImLgdARO3F7vNnYiIyJIYbkyovrcTnGxtNJ9trOQIruEGG6vil91RaY39H7TTWnbjQRYAYHCrANQucuv45K3/YM3heOz45w5OJjxEdp4Kn/56FnWn7sSwImEl9sr9UtUZcy4JAKBWa99m/t8/r+tqjodZ+fi+yLrFezlURURE5QeHpUzotRLm0pQkwMMBnk5K3E3PBQCcvpkCAOj0nBc2n7ipaXc0/gGOxj/QuY+95+/i+LUHCAlwx5ZTj7fp1MALu88m6dzmnbXH8d93QvHWyr8eHXdGZ9zPyMW86POaNm3rVsUfF+/p3H5RzCWM61S39CdKRERkQgw3JqTQ89LMp3mYlYebD7MBAI2quSDNgFvAX1seizHta2PLyVsAgBUDQtCpwaP3V+UVqKGwliP630SM+O/jXp7CYAMAvZYe1jxY0ElpjaNTImCnsIIQAjUn7dB5zAKVGtY6eqSIiIjMjd9GJqRr+Km0Jm75W/O7c5GhrdJa/L+7mgKq2CPiOU/N8sLA1fl/YUeXok9Mfj3EXzN5WCaTYdEbQehY3xNHJ3fE+mEvaNodvaa7J4mIiMjcGG5M6Mm7ogxx40G21udXmvkZtZ929Twhk8mKLZfLZYiP6qZ3W7kMmNL9Oa1lLwf5YeWg5vB0tkXzAHfN8r4r/kJOvgoHLt7DiesPoVLzNRFERGQZHJYyoXb1qhq8TeF8m0J9Q6sDABb0DsK07g3w2fZz2Hzy8VyaH4eEIrCqIzYev4GryZnYeuqWzu11kclkmNClHuZFX0Df0Opo6OuMKVv/BQB82acpej2vf86QXK4dmupPi9b8XsfTEbvHh+vdnoiIyBRkopK9iTEtLQ0uLi5ITU2Fs7Nzme47YOJ2ze9nPomEg9Lw7Fh0HwBwfmaXYg8CvJeei9E/nsSwNoGIKDK8dD4xDV0WHgQALOvXDO4OCoQa8B6qnHwVfj51C50aeKFKkddD6JOYmoMXomJKXN+6tgeOXnuAvILHt4tvf7c1PJ1sUdWpdMcgIiIy5PubPTcmIJPBqGCji64nHFd1UmLD8LBiy+t7O+PYlAi42tsYNd/H1sYKb7QouadHF28XWzSr7oqTCSk61x+6nFxsWfevDgEA/viwPapXsTe4TiIiIn0YbkxArmOOizHOfFL8YX9PY4nekC2jWiEnX4WbD7Pg726PelOjn74RgLaf78PKgSFoX8+z2BAXERGRsTih2ASe5Xu6cBKyk9K6zHp/zMHWxgq1PZ2gtLbCyoEhWs/4iWzohSndnsObLfyLbffO2uMInLwDDzLzIIRAboHK4GPfTcvBt39cQXJG7tMbExGR5HHOTRkqnC+jsJLj4qyuRu3jj4v3MP/3C5jzShM08C3b+swtLScft1OyUd/78Xlk5hZgy8mbmPbLmRK3GxhWA5+83KhUxzhx/SFeXXYEAFCjij2u33/0VOeto1ri+epuOrcpUKlRoBbYf+EejsY/QGJaNiKe88IrzaohX6WGtVym8w4zIiKyHEO+vxluylBhuFFay3HhM+PCTWUghMCZ22l48etDJba5Nqf7U/dz9nYaun11sMT18VHdsPrwNXy+6wIW9G6KkT+cLHWNc19tjD7NDZt/REREpsNwo4c5wo2djRXOzexSpvuWqiOXk9H3//4qtjw+qluJvScZuQX47LezWFcGb03Xp66XI34f9+h29jO3U5FboEazEnqDiIjItHi3lIVxbmzpvRBYBZO71UdDXxcEeDig1Zy9AICak3bg4x4NEBLgjgY+zpDLZUjNykefb2NxPjFds30dT0fMfqUxXl8eCyelNaJebYwxP5566nEV1nK82qwagmu44YONp3W2uZiUUezWfADY90E71PRwMPKMiYjI1BhuTKCs7paqDORyGYa1rQXg0XBVUR//evap268b9gKqOCo1w1iZuQVoWasKjvzvjegzX24IF3sFTl5/iP5hNeDnalfs9vqik5+FEHjj2z/xVwkvJgWA9vP34+MeDdCjqS/WHbuB8LpV0cjPpXQnTEREJsdhqTJU+K98Z1tr/P2x4bdxE7Dx+A18uOnvp7Z7v1NdjGxXq8SXdQohkK8SRr+8dOWheMz87enhqpC1XIb9H7bDy4sP4+OXGqJHU18AQG6BClYyGV8qSkT0jDjnRg9zhBtXexvETe9cpvuuTC4mpePlxYeRnV/8tvAfh4aiZS0Ps9UihECeSg2ltRXyVWrUmbKzVNuNbFcLy/Zf0Xye3aux3ldhXL2XgdeWx6K6uz1Gt6+NjvX57B8ioqIYbvQwR7hxs7fBKYabZ1KgUmt6O3LyVVi89zLa1q2KFjXdn7KlaQkhEP1vIkb+cBLdG/vghUB3vbe1FxX9Xhtk56ngoLTG+mM3sPJQvN72A8NqYG3sdQBAr+f9MO+1JponT6dk5eHM7TS8EFgFVgxBRFQJMNzoYY5wU8VBgRPTOpXpvqn8Wrz3Eub/fhFdGnoj+kyiWY89uFUAeof44zmfiv1MJCKipzHk+7tcTARYsmQJAgICYGtri9DQUBw9erRU261btw4ymQw9e/Y0bYEG4gPgKpcxHerg2pzuWN4/GOc+7YJfx7TG2rdb4OrsbqV6Xg8ALO3XDOdndsGHkfUMOvbqw9fQddFBBEzcjhe/PogcHUN5JVGrRbFJ3EREUmDxnpv169djwIABWL58OUJDQ7Fw4UJs3LgRFy5cgKenZ4nbXbt2Da1bt0ZgYCDc3d3x888/l+p45ui58XRS4uiUiDLdN1Vc15Izse7YDfi42EKlFvBxsUVwgBs8nWx1thdCQC2gGW7KyitA/5VHceL6w1Ifc2LX+njleT94OCqR978nMjsqrSGEQHJGHhbsvoCfjj56TlDRv6+F/ztgQCei8qZCDUuFhoaiefPmWLx4MQBArVbD398f//nPfzBx4kSd26hUKrRt2xZvv/02Dh48iJSUlHIVbrydbfHn5I5lum+ioh5k5mH32UT8Ff8Au88kIT23oEz3f3xqBDwczf8SViKiklSYYam8vDycOHECERGPeznkcjkiIiIQGxtb4naffvopPD098c477zz1GLm5uUhLS9P6MTVHWz4+iEzL3UGBPs2rY0HvIPzzSSSOTOxg0PbP+Tijqb9rietDPtuDI1eScSrhIYQQUKkF8grUz1g1EZF5WPRbODk5GSqVCl5eXlrLvby8cP78eZ3bHDp0CCtXrkRcXFypjhEVFYVPPvnkWUstleVvNcOC3Rfx9ZvNzHI8okK+rnaa+T3ZeSqcvZOK72OvQ2ltBblchp+OJsDP1Q42VjIMD6+FPiH+kMtluPkwC63n7gOg/eJRAOi7ovhrMd7tWAdvhVZHWk4BalV14PAVEZVLFh2Wun37Nvz8/HDkyBGEhYVplk+YMAEHDhzAX39p/881PT0dTZo0wdKlS9G166MXUw4aNEjvsFRubi5yc3M1n9PS0uDv72+SYSmiiu7E9Qd4dVnJvaZFNfR1xsqBzXH9fiYK1AK3HmbjxaY+sFew55KIyl6FebeUh4cHrKyskJSUpLU8KSkJ3t7exdpfuXIF165dQ48ePTTL1OpHXeXW1ta4cOECatWqpbWNUqmEUsm5A0SlEVzDHdfmdMf5xDQIAfz3z0e9P2uOxEP9xD+DztxOwwtRMVrLJmz+G8PaBmJS1/rs1SEiiykXE4pbtGiBr7/+GsCjsFK9enWMGTOm2ITinJwcXL58WWvZ1KlTkZ6ejkWLFqFu3bpQKBR6j2fKCcVEUqVWC6Rk52PD8Ud3WM3ZqXvY+El73w9HYFVHU5ZGRJVEhem5AYDx48dj4MCBCAkJQYsWLbBw4UJkZmZi8ODBAIABAwbAz88PUVFRsLW1RaNGjbS2d3V1BYBiy4mo7MjlMrg7KDAi/FHP6ICwGljw+0V0fM4LDkoreDgqMWTtcZy9oz1hv8MXB7BxRBhuPcxGVSclWtaqghsPsrHp5E30aOKDOl5OljgdIpI4i4ebPn364N69e5g+fToSExMRFBSE6OhozSTjhIQEyOXl4lmDRPQ/9gprTH2xgdayHWPbIF+lxnvr4rD9nzua5a8v1z2H56uYS5j5ckP0DwswZalEVAlZfFjK3DgsRWR6yRm5CPlsT6naHvqoPaq52Zu4IiKq6CrUQ/zMjeGGyHxupWTjYlI6Gvg4o8fXh/BykC/6htbAt39cxU9HE4q1Z08OEZWE4UYPhhui8qHwid5PWvRGELo39tG8FZ6ICGC40Yvhhqh8UKsFvtp7CUcu38fRaw+KrS/tS0eJqHKoUHdLEVHlJJfL8F5EXbwXAeSr1KgzZafW+geZeXB30P9oByIiXdjvS0QWZ2Mlf/TwwJldNMuazdyNBbsvIj4504KVEVFFxJ4bIio3bG2stD5/FXMJX8VcAgD8PLoVgvS87JOIqBDn3BBRuSKEQM1JO0rVtndINbzYxBcvBFaBwpod0URSxgnFejDcEFUMQgj8cysVLy0+XKr2Mhkw95Um+L9DV9E7xB/vtK7J91sRSQjDjR4MN0QVz4nrDxD9byJWHIwHADT2c8E/t1Kful3c9E5wteekZCIpYLjRg+GGSBpSs/Nx40EWvou9hg3Hb5bYblm/ZujSyBuX7mbAydYa3s62uJeRC08nWzNWS0TPiuFGD4YbImlKzc6H0lqOjNyCUr/64d9PIuGo5H0VRBWBId/fnIFHRJLgYmcDW5tHbyi/Nqc7+oT4P3WblxYfMkNlRGRu/CcLEUnS5O7PIT03Hzv+ScSiN4JQz9sJi/dehrezLf7v0KO5O1fvZWpeA9Gxvie+HRACKzknIRNVdByWIqJK5/SNFLy8RPddWHztA1H5xGEpIiI9mvq7YtWgEJ3rMnILzFwNEZU19twQUaWXnJGrcxKyt7MtEtNyAADdG/ugiqMCVR2VGNy6JiciE5kZX5xJRGQAD0elzuWFwQYAtv9zR/P7F7svAgC+6R+MH/9KQFZeAWb1aoy6Xk6mLZSISoU9N0RE/5OckYuBq47izO00o/fxyUsN0f+FGjh7Jw27ziQir0ANe4U11h1LwJ3UR2FpbMc6GNepblmVTVQp8Dk3ejDcENGziDmXhHfWHn/m/Xg5K/Hd26Go583eHqLSYLjRg+GGiMpCTr4KNlZyWMll2HLyJsZvOK23vYPCCpl5qhLXb3+3NRr6upR1mUSSwXCjB8MNEZlKTr4KG4/fQFUnJdrV84StjVWxNjv+uYNRP5zUu5/Gfi74eXQrPnOHqAiGGz0YboioPLibnoMBK4/ifGJ6iW0uftYVCms+sYMIYLjRi+GGiMqbWynZWLrvMn77+w5Ss/OLrf/7485wtrWxQGVE5QfDjR4MN0RUnl2+m46IBX/oXPd8dVdM7vYcmge4m7kqIstjuNGD4YaIyjuVWmDTiRv4aPM/pd5mdq/G6NHUB07s4SGJYrjRg+GGiCqS388kYtj3JwzaZuuolni+upuJKiKyDIYbPRhuiKiiUqsFvj14FasPx6NrIx/8eysVx68/1LvNwQnt4edqhztpOajioNB5BxdRRcBwowfDDRFJ0f2MXATreD9WUXY2Vvj3k0jeYk4VEsONHgw3RCRlF5PS0fubWKRkFb/rSp9tY1qhgY8zrK146zmVTww3ejDcEFFlUaBS4+ydNDgordHxiwMGb9//hRr49OWGkMnY00OWx3CjB8MNEVVGQggkPMjCyYSHUKkfvQ5ix7+J+PX07aduW83NDjU9HHDwUjL6v1ADg1sFILCqoxmqJnqM4UYPhhsiouLupGZj99kk1PRwwJ3UHEzY9PdTt5nUtT6Gh9cyQ3VEDDd6MdwQET1dTr4Kb685hiNX7j+17fK3gtGlkbcZqqLKjOFGD4YbIiLj3XyYhaHfncC5O2lay13sbLBiQAha1OTTk8k0GG70YLghInp2aTn5mLDxb0SfSdS5/tOXG+LVZtXgoLQ2c2UkVQw3ejDcEBGVndI8QVkuA6JeaQy5TIb7mXnwdrZFl0befKAgGYThRg+GGyKisieEwKKYS1i451Kpt7k2p7sJKyKpYbjRg+GGiMj0rtzLwPex17HmyLUS29T1csS2Ma3Zg0OlwnCjB8MNEZFlBUzcrnP5ldnd+GoIKhHDjR4MN0RElnU+MQ1dFh4scb2T0hpD2gRibEQdM1ZF5R3DjR4MN0RE5UNGbgHO3k5D729i9bZzsrVGek4BAGBx3+fxYhNfc5RH5QzDjR4MN0RE5c/1+5lYe+Q6zt5JxZ9XHxi0bRUHBZTWcgR4OKC6uz3a1/dEh/qesOFLQCWF4UYPhhsiovItI7cAW0/dwtH4B6jmZgc3exvs+CcRcTdSjN6nr4stNowIQzU3+7IrlMyK4UYPhhsioopJCIEjV+5jUcwlBPm7IiUrDycTUnD5bgaqOilxLz231Psa3CoA07o3gJwTmCsMhhs9GG6IiKQtJ1+FPy7ew1/xD3A+MQ2HL+t/P9ZrwdXQqnYV9GjiC2sOZZVbDDd6MNwQEVU+DzPzMHD1UdxJzXlqD88rzfyQlavCB5H1EOjhwN6dcoLhRg+GGyIi+uloAiZt+cfg7Ua1q4U3mldH9Sqcu2NuDDd6MNwQEdGTbqVko+eSwxBCIDkj76nt573WBL1D/M1QGRViuNGD4YaIiJ5GCIH7mXlYf+wGjlxJLnHeTq/n/fD5a004V8cMGG70YLghIiJj7b9wF4NWHytxfZs6Hlg5sDkU1gw7ZY3hRg+GGyIiehb30nPRddEfpRq+KvTj0FCEBVaBTMbJycZiuNGD4YaIiMpKwv0snEx4iL/iH+Cnowml2mZUu1p4PcQf7g4KWMtliE/OxN30HDSp5goPR6WJK664GG70YLghIiJTOZnwEHfTcqG0luN2ajambP3XqP38ODQULWt5lHF1FRvDjR4MN0REZG4//pWAr2IuITEtp9TbONtaI+b9dqjqxN4cgOFGL4YbIiKytJx8FQrUAvkFarg5KCCEwPX7Wfj0t7PYe/6uVtvQmu4Y0a4W2tfztFC15QPDjR4MN0REVJ7dS89F81l7dK57L6IOBreqCRc7GzNXZXkMN3ow3BARUUWw5eRNjN9wusT1bep44KMu9dHIz8WMVVkOw40eDDdERFSRJGfkot+Kv3AhKb3ENlWdlJj5ckN0aeRjxsrMi+FGD4YbIiKqqHLyVVgUcwnL9l/R206Kd1sx3OjBcENERFIghMDivZex899EnL2TprNN5wZeSM7Ixah2tRHRwMvMFZYthhs9GG6IiEhqhBA4czsNryw9gjyVWmebam52OPRRBzNXVnYM+f7myy+IiIgqOJlMhkZ+Lrg4qys2DA8D8CjMFHXzYTYCJm7HsWsPLFGiWbHnhoiISMIeZOah2czdxZZ7O9tiYtf6eKmpL+Ty8v/OK/bcEBEREQDA3UGB8zO7oEk17VvGE9Ny8N76OARO3oH//nndQtWZBntuiIiIKgkhBD7Y+Dc2n7xZbF27elWxZnALC1RVOpxQrAfDDREREZCvUuOnowmY/suZYuvebOGPGT0awtbGygKV6cZwowfDDRER0WP/3krFi18f0rmuQ31PrBrU3MwV6cY5N0RERFQqjfxccGV2N6wYEIJOTzwLZ+/5u0i4n2WhyozHnhsiIiLScuzaA7y+PBYAEOTvio0jwmBjZdn+kArXc7NkyRIEBATA1tYWoaGhOHr0aIltV6xYgTZt2sDNzQ1ubm6IiIjQ256IiIgM0zzAXfN73I0U1JmyE7dSsi1YkWEsHm7Wr1+P8ePHY8aMGTh58iSaNm2KyMhI3L17V2f7/fv3480338S+ffsQGxsLf39/dO7cGbdu3TJz5URERNL18+hWWp9bzdmL9Jx8C1VjGIsPS4WGhqJ58+ZYvHgxAECtVsPf3x//+c9/MHHixKdur1Kp4ObmhsWLF2PAgAFPbc9hKSIiotJrPXcvbj583GsT8344alV1NHsdFWZYKi8vDydOnEBERIRmmVwuR0REBGJjY0u1j6ysLOTn58Pd3V3n+tzcXKSlpWn9EBERUelEv9dW63PHLw5YqJLSs2i4SU5OhkqlgpeX9uxsLy8vJCYmlmofH330EXx9fbUCUlFRUVFwcXHR/Pj7+z9z3URERJWFo9Ia1+Z0xwuBjzsRLt9Nt2BFT2fxOTfPYs6cOVi3bh22bt0KW1tbnW0mTZqE1NRUzc+NGzfMXCUREVHF98OQFzS/Ryz4AylZeRasRj+LhhsPDw9YWVkhKSlJa3lSUhK8vb31bjt//nzMmTMHv//+O5o0aVJiO6VSCWdnZ60fIiIiMoyVXIYeTX01n4M+3Y0bD8rnM3AsGm4UCgWCg4MRExOjWaZWqxETE4OwsLASt5s3bx5mzpyJ6OhohISEmKNUIiKiSu/rN59Hl4aPOx/azNuHE9cfWrAi3Sw+LDV+/HisWLECa9euxblz5zBy5EhkZmZi8ODBAIABAwZg0qRJmvZz587FtGnTsGrVKgQEBCAxMRGJiYnIyMiw1CkQERFVGsv7B8PDUaH5/OqyIxasRjeLh5s+ffpg/vz5mD59OoKCghAXF4fo6GjNJOOEhATcuXNH037ZsmXIy8vDa6+9Bh8fH83P/PnzLXUKRERElcrxqZ3wUpEhqoeZ5Wv+jcWfc2NufM4NERHRs8srUKPu1J2azy1rVcGPQ1/Qs8WzqTDPuSEiIqKKSWEth7fz4zuVj1y5j5H/PWHBih5juCEiIiKj/Dm5IzYMf3wD0M5/E8vFEBXDDRERERmtRU13HJnYQfN5UcwlC1bzCMMNERERPRNfVzvN72uOXLNcIf/DcENERETPbGbPRprf76XnWrAShhsiIiIqA31bVNf8npSWY8FKGG6IiIioDFjJZfBxeXT3lKWfWsxwQ0RERGXiTuqjHpsv91y0aB0MN0RERFQmBrUMAAAEejhYtA6GGyIiIioTrWt7QGkth42VZeMFX79ARERE5R5fv0BERESVFsMNERERSQrDDREREUkKww0RERFJCsMNERERSQrDDREREUkKww0RERFJCsMNERERSQrDDREREUkKww0RERFJCsMNERERSQrDDREREUkKww0RERFJCsMNERERSYq1pQswNyEEgEevTiciIqKKofB7u/B7XJ9KF27S09MBAP7+/hauhIiIiAyVnp4OFxcXvW1kojQRSELUajVu374NJycnyGSyMt13Wloa/P39cePGDTg7O5fpvukxXmfz4HU2D15n8+G1Ng9TXWchBNLT0+Hr6wu5XP+smkrXcyOXy1GtWjWTHsPZ2Zn/4ZgBr7N58DqbB6+z+fBam4cprvPTemwKcUIxERERSQrDDREREUkKw00ZUiqVmDFjBpRKpaVLkTReZ/PgdTYPXmfz4bU2j/JwnSvdhGIiIiKSNvbcEBERkaQw3BAREZGkMNwQERGRpDDcEBERkaQw3BhoyZIlCAgIgK2tLUJDQ3H06FG97Tdu3Ij69evD1tYWjRs3xo4dO8xUacVmyHVesWIF2rRpAzc3N7i5uSEiIuKpfy70iKF/nwutW7cOMpkMPXv2NG2BEmHodU5JScHo0aPh4+MDpVKJunXr8v8dpWDodV64cCHq1asHOzs7+Pv7Y9y4ccjJyTFTtRXTH3/8gR49esDX1xcymQw///zzU7fZv38/mjVrBqVSidq1a2PNmjUmrxOCSm3dunVCoVCIVatWiTNnzoihQ4cKV1dXkZSUpLP94cOHhZWVlZg3b544e/asmDp1qrCxsRH//POPmSuvWAy9zn379hVLliwRp06dEufOnRODBg0SLi4u4ubNm2auvGIx9DoXio+PF35+fqJNmzbi5ZdfNk+xFZih1zk3N1eEhISIbt26iUOHDon4+Hixf/9+ERcXZ+bKKxZDr/MPP/wglEql+OGHH0R8fLzYtWuX8PHxEePGjTNz5RXLjh07xJQpU8SWLVsEALF161a97a9evSrs7e3F+PHjxdmzZ8XXX38trKysRHR0tEnrZLgxQIsWLcTo0aM1n1UqlfD19RVRUVE62/fu3Vt0795da1loaKgYPny4Seus6Ay9zk8qKCgQTk5OYu3ataYqURKMuc4FBQWiZcuW4v/+7//EwIEDGW5KwdDrvGzZMhEYGCjy8vLMVaIkGHqdR48eLTp06KC1bPz48aJVq1YmrVNKShNuJkyYIBo2bKi1rE+fPiIyMtKElQnBYalSysvLw4kTJxAREaFZJpfLERERgdjYWJ3bxMbGarUHgMjIyBLbk3HX+UlZWVnIz8+Hu7u7qcqs8Iy9zp9++ik8PT3xzjvvmKPMCs+Y67xt2zaEhYVh9OjR8PLyQqNGjTB79myoVCpzlV3hGHOdW7ZsiRMnTmiGrq5evYodO3agW7duZqm5srDU92Cle3GmsZKTk6FSqeDl5aW13MvLC+fPn9e5TWJios72iYmJJquzojPmOj/po48+gq+vb7H/oOgxY67zoUOHsHLlSsTFxZmhQmkw5jpfvXoVe/fuRb9+/bBjxw5cvnwZo0aNQn5+PmbMmGGOsiscY65z3759kZycjNatW0MIgYKCAowYMQKTJ082R8mVRknfg2lpacjOzoadnZ1JjsueG5KUOXPmYN26ddi6dStsbW0tXY5kpKeno3///lixYgU8PDwsXY6kqdVqeHp64ttvv0VwcDD69OmDKVOmYPny5ZYuTVL279+P2bNnY+nSpTh58iS2bNmC7du3Y+bMmZYujcoAe25KycPDA1ZWVkhKStJanpSUBG9vb53beHt7G9SejLvOhebPn485c+Zgz549aNKkiSnLrPAMvc5XrlzBtWvX0KNHD80ytVoNALC2tsaFCxdQq1Yt0xZdARnz99nHxwc2NjawsrLSLHvuueeQmJiIvLw8KBQKk9ZcERlznadNm4b+/ftjyJAhAIDGjRsjMzMTw4YNw5QpUyCX89/+ZaGk70FnZ2eT9doA7LkpNYVCgeDgYMTExGiWqdVqxMTEICwsTOc2YWFhWu0BYPfu3SW2J+OuMwDMmzcPM2fORHR0NEJCQsxRaoVm6HWuX78+/vnnH8TFxWl+XnrpJbRv3x5xcXHw9/c3Z/kVhjF/n1u1aoXLly9rwiMAXLx4ET4+Pgw2JTDmOmdlZRULMIWBUvCVi2XGYt+DJp2uLDHr1q0TSqVSrFmzRpw9e1YMGzZMuLq6isTERCGEEP379xcTJ07UtD98+LCwtrYW8+fPF+fOnRMzZszgreClYOh1njNnjlAoFGLTpk3izp07mp/09HRLnUKFYOh1fhLvliodQ69zQkKCcHJyEmPGjBEXLlwQv/32m/D09BSfffaZpU6hQjD0Os+YMUM4OTmJn376SVy9elX8/vvvolatWqJ3796WOoUKIT09XZw6dUqcOnVKABALFiwQp06dEtevXxdCCDFx4kTRv39/TfvCW8E//PBDce7cObFkyRLeCl4eff3116J69epCoVCIFi1aiD///FOzLjw8XAwcOFCr/YYNG0TdunWFQqEQDRs2FNu3bzdzxRWTIde5Ro0aAkCxnxkzZpi/8ArG0L/PRTHclJ6h1/nIkSMiNDRUKJVKERgYKGbNmiUKCgrMXHXFY8h1zs/PFx9//LGoVauWsLW1Ff7+/mLUqFHi4cOH5i+8Atm3b5/O/98WXtuBAweK8PDwYtsEBQUJhUIhAgMDxerVq01ep0wI9r8RERGRdHDODREREUkKww0RERFJCsMNERERSQrDDREREUkKww0RERFJCsMNERERSQrDDREREUkKww0REQCZTIaff/4ZAHDt2jXIZDK+AZ2ogmK4ISKLGzRoEGQyGWQyGWxsbFCzZk1MmDABOTk5li6NiCogvhWciMqFLl26YPXq1cjPz8eJEycwcOBAyGQyzJ0719KlEVEFw54bIioXlEolvL294e/vj549eyIiIgK7d+8G8OgNz1FRUahZsybs7OzQtGlTbNq0SWv7M2fO4MUXX4SzszOcnJzQpk0bXLlyBQBw7NgxdOrUCR4eHnBxcUF4eDhOnjxp9nMkIvNguCGicufff//FkSNHoFAoAABRUVH47rvvsHz5cpw5cwbjxo3DW2+9hQMHDgAAbt26hbZt20KpVGLv3r04ceIE3n77bRQUFAAA0tPTMXDgQBw6dAh//vkn6tSpg27duiE9Pd1i50hEpsNhKSIqF3777Tc4OjqioKAAubm5kMvlWLx4MXJzczF79mzs2bMHYWFhAIDAwEAcOnQI33zzDcLDw7FkyRK4uLhg3bp1sLGxAQDUrVtXs+8OHTpoHevbb7+Fq6srDhw4gBdffNF8J0lEZsFwQ0TlQvv27bFs2TJkZmbiyy+/hLW1NV599VWcOXMGWVlZ6NSpk1b7vLw8PP/88wCAuLg4tGnTRhNsnpSUlISpU6di//79uHv3LlQqFbKyspCQkGDy8yIi82O4IaJywcHBAbVr1wYArFq1Ck2bNsXKlSvRqFEjAMD27dvh5+entY1SqQQA2NnZ6d33wIEDcf/+fSxatAg1atSAUqlEWFgY8vLyTHAmRGRpDDdEVO7I5XJMnjwZ48ePx8WLF6FUKpGQkIDw8HCd7Zs0aYK1a9ciPz9fZ+/N4cOHsXTpUnTr1g0AcOPGDSQnJ5v0HIjIcjihmIjKpddffx1WVlb45ptv8MEHH2DcuHFYu3Ytrly5gpMnT+Lrr7/G2rVrAQBjxoxBWloa3njjDRw/fhyXLl3C999/jwsXLgAA6tSpg++//x7nzp3DX3/9hX79+j21t4eIKi723BBRuWRtbY0xY8Zg3rx5iI+PR9WqVREVFYWrV6/C1dUVzZo1w+TJkwEAVapUwd69e/Hhhx8iPDwcVlZWCAoKQqtWrQAAK1euxLBhw9CsWTP4+/tj9uzZ+OCDDyx5ekRkQjIhhLB0EURERERlhcNSREREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKf8Pr0tClnnPkxsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gda_train_nd(X, y):\n",
    "    classes = np.unique(y)\n",
    "    priors, means = {}, {}\n",
    "    n, d = X.shape\n",
    "    Sigma = np.zeros((d, d))\n",
    "    for c in classes:\n",
    "        Xc = X[y==c]\n",
    "        priors[c] = len(Xc)/n\n",
    "        means[c] = np.mean(Xc, axis=0)\n",
    "        Sigma += (Xc - means[c]).T @ (Xc - means[c])\n",
    "    Sigma /= n\n",
    "    return priors, means, Sigma\n",
    "\n",
    "def gda_predict_nd(X, priors, means, Sigma):\n",
    "    invS = np.linalg.inv(Sigma)\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        scores = {}\n",
    "        for c in priors:\n",
    "            mu = means[c]\n",
    "            scores[c] = x @ invS @ mu - 0.5*mu @ invS @ mu + np.log(priors[c])\n",
    "        preds.append(max(scores, key=scores.get))\n",
    "    return np.array(preds)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "precisions, recalls, f1s, accuracies = [], [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_ex2):\n",
    "    X_train, X_test = X_ex2[train_idx], X_ex2[test_idx]\n",
    "    y_train, y_test = y_ex2[train_idx], y_ex2[test_idx]\n",
    "\n",
    "    priors, means, Sigma = gda_train_nd(X_train, y_train)\n",
    "    y_pred = gda_predict_nd(X_test, priors, means, Sigma)\n",
    "\n",
    "    precisions.append(precision_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1s.append(f1)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "gda_nd_cv_results = {\n",
    "    \"Model\": \"GDA nD (10-fold CV)\",\n",
    "    \"Accuracy\": f\"{np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\",\n",
    "    \"Precision\": f\"{np.mean(precisions):.3f} ± {np.std(precisions):.3f}\",\n",
    "    \"Recall\": f\"{np.mean(recalls):.3f} ± {np.std(recalls):.3f}\",\n",
    "    \"F1\": f\"{np.mean(f1s):.3f} ± {np.std(f1s):.3f}\"\n",
    "}\n",
    "\n",
    "results.append(gda_nd_cv_results)\n",
    "\n",
    "print(\"10-Fold CV Results (GDA nD):\")\n",
    "for k, v in gda_nd_cv_results.items():\n",
    "    if k != \"Model\":\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "\n",
    "def gda_predict_proba(X, priors, means, Sigma):\n",
    "    invS = np.linalg.inv(Sigma)\n",
    "    probs = []\n",
    "    for x in X:\n",
    "        g = []\n",
    "        for c in sorted(priors.keys()):\n",
    "            mu = means[c]\n",
    "            val = x @ invS @ mu - 0.5*mu @ invS @ mu + np.log(priors[c])\n",
    "            g.append(val)\n",
    "        g = np.exp(g - np.max(g))  # softmax trick\n",
    "        g /= g.sum()\n",
    "        probs.append(g)\n",
    "    return np.array(probs)\n",
    "\n",
    "# retrain with all data so we can do the plot \n",
    "# Train/test split\n",
    "X_train_ex2, X_test_ex2, y_train_ex2, y_test_ex2 = train_test_split(X_ex2, y_ex2, test_size=0.2, random_state=42)\n",
    "priors, means, Sigma = gda_train_nd(X_train_ex2, y_train_ex2)\n",
    "y_scores = gda_predict_proba(X_test_ex2, priors, means, Sigma)[:, 1]\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test_ex2, y_scores)\n",
    "auc_val = auc(recalls, precisions)\n",
    "\n",
    "plt.plot(recalls, precisions, label=f\"AUC={auc_val:.3f}\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59464d",
   "metadata": {},
   "source": [
    "The PR curve shows that precision is high only when recall is very low. As recall increases, precision drops quickly, reflecting difficulty distinguishing high-income individuals. The AUC of 0.354 indicates the model performs only slightly above random chance. This is consistent with the imbalance in the Adult dataset and the restrictive Gaussian assumptions of GDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c09b0b",
   "metadata": {},
   "source": [
    "### nD k-Class Gaussian Discriminant Analysis \n",
    "\n",
    "**Prepare Dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "017efcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (178, 13)\n",
      "y classes: [0 1 2]\n",
      "Class counts: {0: 59, 1: 71, 2: 48}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fetch Wine dataset\n",
    "wine = fetch_ucirepo(id=109)\n",
    "X_df_ex3 = wine.data.features\n",
    "y_df_ex3 = wine.data.targets\n",
    "\n",
    "# Convert target to numeric vector (0,1,2)\n",
    "le = LabelEncoder()\n",
    "y_ex3 = le.fit_transform(y_df_ex3.iloc[:,0])  # assumes only one column in targets\n",
    "\n",
    "print(\"X shape:\", X_df_ex3.shape)   # (178, 13)\n",
    "print(\"y classes:\", np.unique(y_ex3))\n",
    "\n",
    "unique, counts = np.unique(y_ex3, return_counts=True)\n",
    "class_counts = {int(u): int(c) for u, c in zip(unique, counts)}\n",
    "print(\"Class counts:\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8a5e6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gda_train_nd(X, y):\n",
    "    classes = np.unique(y)\n",
    "    priors, means = {}, {}\n",
    "    n, d = X.shape\n",
    "    Sigma = np.zeros((d, d))\n",
    "    for c in classes:\n",
    "        Xc = X[y==c]\n",
    "        priors[c] = len(Xc)/n\n",
    "        mu = Xc.mean(axis=0)\n",
    "        means[c] = mu\n",
    "        Sigma += (Xc - mu).T @ (Xc - mu)\n",
    "    Sigma /= n\n",
    "    return priors, means, Sigma\n",
    "\n",
    "def gda_predict_nd(X, priors, means, Sigma):\n",
    "    invS = np.linalg.inv(Sigma)\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        scores = {c: x @ invS @ mu - 0.5 * mu @ invS @ mu + np.log(priors[c]) for c, mu in means.items()}\n",
    "        preds.append(max(scores, key=scores.get))\n",
    "    return np.array(preds)\n",
    "\n",
    "def gda_predict_proba(X, priors, means, Sigma):\n",
    "    invS = np.linalg.inv(Sigma)\n",
    "    classes = sorted(priors.keys())\n",
    "    probs = []\n",
    "    for x in X:\n",
    "        g = []\n",
    "        for c in classes:\n",
    "            mu = means[c]\n",
    "            val = x @ invS @ mu - 0.5 * mu @ invS @ mu + np.log(priors[c])\n",
    "            g.append(val)\n",
    "        g = np.exp(g - np.max(g))  # stability\n",
    "        g /= g.sum()\n",
    "        probs.append(np.array(g))\n",
    "    return np.vstack(probs), classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da812c9",
   "metadata": {},
   "source": [
    "**Estimate parameters, compute discriminant function for each class, classify examples and measure error :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a4cc5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Fold CV Results (GDA nD k-Class):\n",
      "Accuracy: 0.994 ± 0.017\n",
      "Precision: 0.992 ± 0.025\n",
      "Recall: 0.996 ± 0.012\n",
      "F1: 0.993 ± 0.021\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_ex3 = X_df_ex3.values\n",
    "# used later in ex6\n",
    "X_train_ex3, X_test_ex3, y_train_ex3, y_test_ex3 = train_test_split(\n",
    "    X_ex3, y_ex3, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "precisions, recalls, f1s, accuracies = [], [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_ex3):\n",
    "    X_train, X_test = X_ex3[train_idx], X_ex3[test_idx]\n",
    "    y_train, y_test = y_ex3[train_idx], y_ex3[test_idx]\n",
    "\n",
    "    priors, means, Sigma = gda_train_nd(X_train, y_train)\n",
    "    y_pred = gda_predict_nd(X_test, priors, means, Sigma)\n",
    "\n",
    "    precisions.append(precision_score(y_test, y_pred, average='macro'))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "gda_nd_kc_cv_results = {\n",
    "    \"Model\": \"GDA nD k-Class (10-fold CV)\",\n",
    "    \"Accuracy\": f\"{np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\",\n",
    "    \"Precision\": f\"{np.mean(precisions):.3f} ± {np.std(precisions):.3f}\",\n",
    "    \"Recall\": f\"{np.mean(recalls):.3f} ± {np.std(recalls):.3f}\",\n",
    "    \"F1\": f\"{np.mean(f1s):.3f} ± {np.std(f1s):.3f}\"\n",
    "}\n",
    "\n",
    "results.append(gda_nd_kc_cv_results)\n",
    "\n",
    "print(\"10-Fold CV Results (GDA nD k-Class):\")\n",
    "for k, v in gda_nd_kc_cv_results.items():\n",
    "    if k != \"Model\":\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5dcf81",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes with Bernoulli Features\n",
    "\n",
    "**Prepare dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c4a21245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (4601, 57)\n",
      "y classes: [0 1]\n"
     ]
    }
   ],
   "source": [
    "spambase = fetch_ucirepo(id=94)\n",
    "\n",
    "X_df_ex4 = spambase.data.features\n",
    "y_df_ex4 = spambase.data.targets\n",
    "\n",
    "X_ex4 = X_df_ex4.values\n",
    "y_ex4 = y_df_ex4.values.ravel()\n",
    "\n",
    "print(\"X shape:\", X_ex4.shape)   # (4601, 57)\n",
    "print(\"y classes:\", np.unique(y_ex4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6955b5",
   "metadata": {},
   "source": [
    "**Estimate parameters, compute discriminant function for each class, classify examples and measure error :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3d6c01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_bernoulli_train(X, y, alpha=1.0):\n",
    "    classes = np.unique(y)\n",
    "    priors = {}\n",
    "    cond_probs = {}\n",
    "    for c in classes:\n",
    "        Xc = X[y==c]\n",
    "        priors[c] = len(Xc) / len(X)\n",
    "        theta = (Xc.sum(axis=0) + alpha) / (Xc.shape[0] + 2*alpha)\n",
    "        cond_probs[c] = theta\n",
    "    return priors, cond_probs\n",
    "\n",
    "def nb_bernoulli_predict(X, priors, cond_probs):\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        scores = {}\n",
    "        for c in priors:\n",
    "            theta = cond_probs[c]\n",
    "            log_prob = np.log(priors[c]) + (x * np.log(theta) + (1-x) * np.log(1-theta)).sum()\n",
    "            scores[c] = log_prob\n",
    "        preds.append(max(scores, key=scores.get))\n",
    "    return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5e08f66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Fold CV Results (Bernoulli NB):\n",
      "Accuracy: 0.885 ± 0.016\n",
      "Precision: 0.884 ± 0.026\n",
      "Recall: 0.816 ± 0.038\n",
      "F1: 0.848 ± 0.022\n"
     ]
    }
   ],
   "source": [
    "X_bin_ex4 = (X_ex4 > 0).astype(int) \n",
    "X_train_ex4, X_test_ex4, y_train_ex4, y_test_ex4 = train_test_split( X_bin_ex4, y_ex4, test_size=0.2, random_state=42, stratify=y_ex4)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "precisions, recalls, f1s, accuracies = [], [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_bin_ex4):\n",
    "    X_train, X_test = X_bin_ex4[train_idx], X_bin_ex4[test_idx]\n",
    "    y_train, y_test = y_ex4[train_idx], y_ex4[test_idx]\n",
    "\n",
    "    priors, cond_probs = nb_bernoulli_train(X_train, y_train)\n",
    "    y_pred= nb_bernoulli_predict(X_test, priors, cond_probs)\n",
    "\n",
    "    precisions.append(precision_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1s.append(f1)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "nb_bernoulli_results = {\n",
    "    \"Model\": \"Bernoulli NB (10-fold CV)\",\n",
    "    \"Accuracy\": f\"{np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\",\n",
    "    \"Precision\": f\"{np.mean(precisions):.3f} ± {np.std(precisions):.3f}\",\n",
    "    \"Recall\": f\"{np.mean(recalls):.3f} ± {np.std(recalls):.3f}\",\n",
    "    \"F1\": f\"{np.mean(f1s):.3f} ± {np.std(f1s):.3f}\"\n",
    "}\n",
    "\n",
    "results.append(nb_bernoulli_results)\n",
    "\n",
    "print(\"10-Fold CV Results (Bernoulli NB):\")\n",
    "for k, v in nb_bernoulli_results.items():\n",
    "    if k != \"Model\":\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36b866",
   "metadata": {},
   "source": [
    "### 5. Naive Bayes with Binomial Features\n",
    "\n",
    "**Prepare dataset** : already done in ex4, using the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730973f",
   "metadata": {},
   "source": [
    "**Estimate parameters, compute discriminant function for each class, classify examples and measure error :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7e4d2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_binomial_train(X, y, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Multinomial Naive Bayes training.\n",
    "    Each feature is a discrete count.\n",
    "    \"\"\"\n",
    "    classes = np.unique(y)\n",
    "    priors = {}\n",
    "    cond_probs = {}\n",
    "    for c in classes:\n",
    "        Xc = X[y == c]  # subset with class c\n",
    "        priors[c] = len(Xc) / len(X)\n",
    "        # total word counts across docs of class c\n",
    "        word_count = Xc.sum(axis=0) + alpha\n",
    "        total_count = word_count.sum()\n",
    "        theta = word_count / total_count\n",
    "        cond_probs[c] = theta\n",
    "    return priors, cond_probs\n",
    "\n",
    "\n",
    "# 4) Prediction with discriminant function\n",
    "def nb_binomial_predict(X, priors, cond_probs):\n",
    "    classes = list(priors.keys())\n",
    "    preds = np.empty(X.shape[0], dtype=classes[0].__class__)\n",
    "    for i, x in enumerate(X):\n",
    "        best_c, best_score = None, -np.inf\n",
    "        for c in classes:\n",
    "            theta = cond_probs[c]\n",
    "            # discriminant: log prior + sum_j x_j log theta_j|c\n",
    "            score = np.log(priors[c]) + (x * np.log(theta)).sum()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_c = c\n",
    "        preds[i] = best_c\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0fb08034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Fold CV Results (Binomial NB):\n",
      "Accuracy: 0.791 ± 0.020\n",
      "Precision: 0.741 ± 0.039\n",
      "Recall: 0.722 ± 0.036\n",
      "F1: 0.730 ± 0.031\n"
     ]
    }
   ],
   "source": [
    "X_train_ex5, X_test_ex5, y_train_ex5, y_test_ex5 = train_test_split(\n",
    "    X_ex4, y_ex4, test_size=0.2, random_state=42, stratify=y_ex4\n",
    ")\n",
    "\n",
    "\n",
    "X_bin_ex4 = (X_ex4 > 0).astype(int) \n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "precisions, recalls, f1s, accuracies = [], [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_ex4):\n",
    "    X_train, X_test = X_ex4[train_idx], X_ex4[test_idx]\n",
    "    y_train, y_test = y_ex4[train_idx], y_ex4[test_idx]\n",
    "\n",
    "    priors, cond_probs = nb_binomial_train(X_train, y_train, alpha=1.0)\n",
    "    y_pred= nb_binomial_predict(X_test, priors, cond_probs)\n",
    "\n",
    "    precisions.append(precision_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1s.append(f1)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "nb_binomial_results = {\n",
    "    \"Model\": \"Binomial NB (10-fold CV)\",\n",
    "    \"Accuracy\": f\"{np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\",\n",
    "    \"Precision\": f\"{np.mean(precisions):.3f} ± {np.std(precisions):.3f}\",\n",
    "    \"Recall\": f\"{np.mean(recalls):.3f} ± {np.std(recalls):.3f}\",\n",
    "    \"F1\": f\"{np.mean(f1s):.3f} ± {np.std(f1s):.3f}\"\n",
    "}\n",
    "\n",
    "results.append(nb_binomial_results)\n",
    "\n",
    "print(\"10-Fold CV Results (Binomial NB):\")\n",
    "for k, v in nb_binomial_results.items():\n",
    "    if k != \"Model\":\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0985b3a",
   "metadata": {},
   "source": [
    "### 6. Comparison with baselines\n",
    "\n",
    "**Gaussian discriminant analysis**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4106fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Discriminant Analysis \n",
    "\n",
    "# Skelearn LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_ex1, y_train_ex1)\n",
    "y_pred_lda_ex1 = lda.predict(X_test_ex1)\n",
    "\n",
    "\n",
    "# Sklearn LDA nD\n",
    "lda2 = LinearDiscriminantAnalysis()\n",
    "lda2.fit(X_train_ex2, y_train_ex2)\n",
    "y_pred_lda_ex2 = lda2.predict(X_test_ex2)\n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_ex3, y_train_ex3)\n",
    "y_pred_lda_ex3 = lda.predict(X_test_ex3)\n",
    "\n",
    "\n",
    "bnb = BernoulliNB(alpha=1.0)\n",
    "bnb.fit(X_train_ex4, y_train_ex4)\n",
    "y_pred_nb_ex4 = bnb.predict(X_test_ex4)\n",
    "\n",
    "\n",
    "mnb = MultinomialNB(alpha=1.0)\n",
    "mnb.fit(X_train_ex5, y_train_ex5)\n",
    "y_pred_nb_ex5 = mnb.predict(X_test_ex5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f8c40f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Model       Accuracy      Precision         Recall  \\\n",
      "0   Custom GDA 1D (10-fold CV)  0.692 ± 0.018  0.740 ± 0.019  0.790 ± 0.020   \n",
      "1          GDA nD (10-fold CV)  0.838 ± 0.005  0.477 ± 0.025  0.085 ± 0.006   \n",
      "2  GDA nD k-Class (10-fold CV)  0.994 ± 0.017  0.992 ± 0.025  0.996 ± 0.012   \n",
      "3    Bernoulli NB (10-fold CV)  0.885 ± 0.016  0.884 ± 0.026  0.816 ± 0.038   \n",
      "4     Binomial NB (10-fold CV)  0.791 ± 0.020  0.741 ± 0.039  0.722 ± 0.036   \n",
      "5               Sklearn LDA 1D       0.688462       0.754587       0.775029   \n",
      "6               Sklearn LDA nD       0.837547        0.45583       0.082586   \n",
      "7        Sklearn LDA (k-class)            1.0            1.0            1.0   \n",
      "8          Sklearn BernoulliNB       0.876221       0.871642       0.804408   \n",
      "9        Sklearn MultinomialNB        0.77633       0.719888       0.707989   \n",
      "\n",
      "              F1  \n",
      "0  0.764 ± 0.015  \n",
      "1  0.144 ± 0.010  \n",
      "2  0.993 ± 0.021  \n",
      "3  0.848 ± 0.022  \n",
      "4  0.730 ± 0.031  \n",
      "5       0.764672  \n",
      "6       0.139837  \n",
      "7            1.0  \n",
      "8       0.836676  \n",
      "9       0.713889  \n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(name, y_true, y_pred):\n",
    "    # Detect if it's a multiclass problem\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    if num_classes > 2:\n",
    "        avg_type = 'macro'\n",
    "    else:\n",
    "        avg_type = 'binary'\n",
    "    \n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average=avg_type),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average=avg_type),\n",
    "        \"F1\": f1_score(y_true, y_pred, average=avg_type)\n",
    "    }\n",
    "\n",
    "\n",
    "results.append(evaluate_model(\"Sklearn LDA 1D\", y_test_ex1, y_pred_lda_ex1))\n",
    "results.append(evaluate_model(\"Sklearn LDA nD\", y_test_ex2, y_pred_lda_ex2))\n",
    "results.append(evaluate_model(\"Sklearn LDA (k-class)\", y_test_ex3, y_pred_lda_ex3))\n",
    "results.append(evaluate_model(\"Sklearn BernoulliNB\", y_test_ex4, y_pred_nb_ex4))\n",
    "results.append(evaluate_model(\"Sklearn MultinomialNB\", y_test_ex5, y_pred_nb_ex5))\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c7e0c3",
   "metadata": {},
   "source": [
    "**Brief discussion of differences:**\n",
    "\n",
    "To validate the custom implementations, Gaussian Discriminant Analysis (GDA) and Naive Bayes (NB) models were compared against their scikit-learn counterparts using identical datasets and evaluation procedures. Model performance was evaluated through 10-fold cross-validation, measuring accuracy, precision, recall, and F1-score. The results show strong consistency between the custom and sklearn models across all configurations. In the one-dimensional case, both GDA implementations achieved nearly identical performance (Accuracy ≈ 0.69, F1 ≈ 0.76), confirming the correctness of the discriminant function and shared variance estimation. For the n-dimensional version, the custom GDA closely matched sklearn’s LinearDiscriminantAnalysis, with minor numerical differences. The k-class GDA achieved almost perfect classification on the chosen dataset, in line with sklearn’s LDA, which demonstrates correct generalization to multiple classes.\n",
    "\n",
    "Similarly, the custom Naive Bayes implementations, both Bernoulli and Binomial (Multinomial), produced metrics that were nearly identical to those from sklearn’s BernoulliNB and MultinomialNB models. Deviations below two percent arose from small implementation differences, such as the use of Laplace smoothing (α = 1 in sklearn) and floating point rounding. Overall, the results confirm that all custom classifiers reproduce sklearn’s performance with high fidelity, and that any discrepancies are purely numerical rather than conceptual. This validates the correctness and stability of the mathematical derivations and numerical implementations of both GDA and NB classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4c23e",
   "metadata": {},
   "source": [
    "### 7. Reflection\n",
    "\n",
    "During this project, what I understood most clearly is the role of each generative model and when it should be used. In Gaussian Discriminant Analysis (GDA), the features are continuous, and the class-conditional distributions are modeled as Gaussians. Depending on the covariance structure, we obtain LDA (shared covariance, linear boundaries) or QDA (class-specific covariance, quadratic boundaries). This helped me see how assumptions about the data shape the decision surface.\n",
    "\n",
    "In contrast, Naive Bayes models are used for discrete or categorical data. The Bernoulli version models binary features (presence or absence, yes/no), while the Binomial version models the number of successes in a fixed number of trials. The Multinomial version, often used in text classification, models counts of words or tokens without a fixed limit. Implementing these made me understand that the core idea is always the same estimate P(X∣Y) and use Bayes’ rule to classify, but the choice of distribution depends entirely on the type of data and the information each feature carries.\n",
    "\n",
    "Through coding these models from scratch, I realized how the independence assumptions, the distributional choices, and the estimated parameters directly affect performance and numerical stability. More importantly, I now feel confident about which model to choose for a given dataset and why that assumption makes sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
